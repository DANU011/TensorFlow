{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       user_ID trial_ID task_ID         gx         gy         gz   \n",
      "0         SA01      R01     D01  -1.098633 -30.761719 -21.484375  \\\n",
      "1         SA01      R01     D01 -10.864258 -46.752930  -3.173828   \n",
      "2         SA01      R01     D01  31.860352 -22.216797   8.056641   \n",
      "3         SA01      R01     D01   2.624512 -11.352539  29.052734   \n",
      "4         SA01      R01     D01   7.263184  15.869141  26.184082   \n",
      "...        ...      ...     ...        ...        ...        ...   \n",
      "300097    SE14      R06     D07  -3.479004   2.563477  -0.061035   \n",
      "300098    SE14      R06     D07  -2.197266   3.234863   0.488281   \n",
      "300099    SE14      R06     D07  -4.394531   2.990723   0.549316   \n",
      "300100    SE14      R06     D07  -2.746582   2.563477   0.000000   \n",
      "300101    SE14      R06     D07  -4.028320   2.258301   0.122070   \n",
      "\n",
      "                   label  scaled_gx  scaled_gy  scaled_gz  \n",
      "0                walking  -0.284779  -0.036755   0.042287  \n",
      "1                walking  -0.291049  -0.056733   0.066825  \n",
      "2                walking  -0.263618  -0.026079   0.081875  \n",
      "3                walking  -0.282389  -0.012506   0.110011  \n",
      "4                walking  -0.279411   0.021504   0.106167  \n",
      "...                  ...        ...        ...        ...  \n",
      "300097  standing-sitting  -0.286308   0.004880   0.070996  \n",
      "300098  standing-sitting  -0.285485   0.005719   0.071732  \n",
      "300099  standing-sitting  -0.286896   0.005414   0.071814  \n",
      "300100  standing-sitting  -0.285837   0.004880   0.071078  \n",
      "300101  standing-sitting  -0.286660   0.004499   0.071242  \n",
      "\n",
      "[300102 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data/SisFall_train.csv')\n",
    "\n",
    "print(df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-10T03:48:00.139629800Z",
     "start_time": "2023-06-10T03:47:59.426580600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2342/2342 [==============================] - 6s 2ms/step - loss: 1.5837 - accuracy: 0.2618\n",
      "Epoch 2/100\n",
      "2342/2342 [==============================] - 4s 2ms/step - loss: 1.3703 - accuracy: 0.3883\n",
      "Epoch 3/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 1.2822 - accuracy: 0.4208\n",
      "Epoch 4/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 1.2162 - accuracy: 0.4410\n",
      "Epoch 5/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 1.1779 - accuracy: 0.4601\n",
      "Epoch 6/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 1.1542 - accuracy: 0.4695\n",
      "Epoch 7/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 1.1350 - accuracy: 0.4784\n",
      "Epoch 8/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 1.0925 - accuracy: 0.5002\n",
      "Epoch 9/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 1.0571 - accuracy: 0.5184\n",
      "Epoch 10/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 1.0320 - accuracy: 0.5305\n",
      "Epoch 11/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 1.0116 - accuracy: 0.5440\n",
      "Epoch 12/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.9987 - accuracy: 0.5535\n",
      "Epoch 13/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.9886 - accuracy: 0.5590\n",
      "Epoch 14/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.9828 - accuracy: 0.5624\n",
      "Epoch 15/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.9729 - accuracy: 0.5679\n",
      "Epoch 16/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.9687 - accuracy: 0.5727\n",
      "Epoch 17/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.9601 - accuracy: 0.5774\n",
      "Epoch 18/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.9531 - accuracy: 0.5803\n",
      "Epoch 19/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.9454 - accuracy: 0.5854\n",
      "Epoch 20/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.9412 - accuracy: 0.5856\n",
      "Epoch 21/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.9366 - accuracy: 0.5882\n",
      "Epoch 22/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.9316 - accuracy: 0.5923\n",
      "Epoch 23/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.9270 - accuracy: 0.5960\n",
      "Epoch 24/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.9229 - accuracy: 0.5961\n",
      "Epoch 25/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.9188 - accuracy: 0.5974\n",
      "Epoch 26/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.9160 - accuracy: 0.6011\n",
      "Epoch 27/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.9140 - accuracy: 0.6009\n",
      "Epoch 28/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.9079 - accuracy: 0.6040\n",
      "Epoch 29/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.9066 - accuracy: 0.6043\n",
      "Epoch 30/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.9039 - accuracy: 0.6043\n",
      "Epoch 31/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.8998 - accuracy: 0.6073\n",
      "Epoch 32/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.8974 - accuracy: 0.6079\n",
      "Epoch 33/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.8952 - accuracy: 0.6094\n",
      "Epoch 34/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.8912 - accuracy: 0.6106\n",
      "Epoch 35/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.8887 - accuracy: 0.6142\n",
      "Epoch 36/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.8853 - accuracy: 0.6149\n",
      "Epoch 37/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.8836 - accuracy: 0.6142\n",
      "Epoch 38/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.8819 - accuracy: 0.6155\n",
      "Epoch 39/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.8782 - accuracy: 0.6159\n",
      "Epoch 40/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.8750 - accuracy: 0.6178\n",
      "Epoch 41/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.8730 - accuracy: 0.6178\n",
      "Epoch 42/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.8705 - accuracy: 0.6210\n",
      "Epoch 43/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.8677 - accuracy: 0.6221\n",
      "Epoch 44/100\n",
      "2342/2342 [==============================] - 6s 2ms/step - loss: 0.8655 - accuracy: 0.6222\n",
      "Epoch 45/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.8626 - accuracy: 0.6230\n",
      "Epoch 46/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.8616 - accuracy: 0.6235\n",
      "Epoch 47/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.8587 - accuracy: 0.6252\n",
      "Epoch 48/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.8556 - accuracy: 0.6274\n",
      "Epoch 49/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.8537 - accuracy: 0.6282\n",
      "Epoch 50/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.8515 - accuracy: 0.6267\n",
      "Epoch 51/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.8483 - accuracy: 0.6305\n",
      "Epoch 52/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.8479 - accuracy: 0.6290\n",
      "Epoch 53/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.8452 - accuracy: 0.6312\n",
      "Epoch 54/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.8446 - accuracy: 0.6306\n",
      "Epoch 55/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.8431 - accuracy: 0.6311\n",
      "Epoch 56/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.8393 - accuracy: 0.6332\n",
      "Epoch 57/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.8368 - accuracy: 0.6341\n",
      "Epoch 58/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.8351 - accuracy: 0.6361\n",
      "Epoch 59/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.8347 - accuracy: 0.6353\n",
      "Epoch 60/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.8307 - accuracy: 0.6378\n",
      "Epoch 61/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.8298 - accuracy: 0.6385\n",
      "Epoch 62/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.8299 - accuracy: 0.6390\n",
      "Epoch 63/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.8265 - accuracy: 0.6366\n",
      "Epoch 64/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.8262 - accuracy: 0.6391\n",
      "Epoch 65/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.8235 - accuracy: 0.6392\n",
      "Epoch 66/100\n",
      "2342/2342 [==============================] - 6s 2ms/step - loss: 0.8219 - accuracy: 0.6398\n",
      "Epoch 67/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.8209 - accuracy: 0.6411\n",
      "Epoch 68/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.8172 - accuracy: 0.6437\n",
      "Epoch 69/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.8190 - accuracy: 0.6422\n",
      "Epoch 70/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.8154 - accuracy: 0.6461\n",
      "Epoch 71/100\n",
      "   1/2342 [..............................] - ETA: 7s - loss: 0.5877 - accuracy: 0.7188"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import pickle\n",
    "\n",
    "# 데이터셋 준비\n",
    "X = []  # 입력 데이터\n",
    "y = []  # 출력 데이터\n",
    "\n",
    "# 4개의 레코드씩 묶어서 처리\n",
    "for i in range(0, len(df) - 3, 4):\n",
    "    # gx, gy, gz 값 가져오기\n",
    "    gx_values = df.loc[i:i+3, 'scaled_gx'].values\n",
    "    gy_values = df.loc[i:i+3, 'scaled_gy'].values\n",
    "    gz_values = df.loc[i:i+3, 'scaled_gz'].values\n",
    "\n",
    "    # 레이블 값 가져오기\n",
    "    labels = df.loc[i:i+3, 'label'].values\n",
    "\n",
    "    # 서로 다른 레이블이 포함된 경우 해당 시퀀스는 분석에서 제외\n",
    "    if len(set(labels)) == 1:\n",
    "        # 시퀀스로 변환하여 입력 데이터에 추가\n",
    "        X.append([gx_values, gy_values, gz_values])\n",
    "\n",
    "        # 레이블 값 중복 제거하여 출력 데이터에 추가\n",
    "        y.append(labels[0])\n",
    "\n",
    "# 입력 데이터와 출력 데이터를 넘파이 배열로 변환\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# 출력 데이터를 One-Hot 인코딩\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(y)\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "y = onehot_encoder.fit_transform(integer_encoded)\n",
    "\n",
    "# LSTM 모델 정의\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.LSTM(128, input_shape=(3, 4)))  # 3개의 시퀀스, 각 시퀀스에 4개의 피처\n",
    "model.add(keras.layers.Dense(5, activation='softmax'))  # 분류할 클래스 수에 맞게 조정\n",
    "\n",
    "# 모델 컴파일 및 학습\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X, y, epochs=100, batch_size=32)\n",
    "\n",
    "# 최상의 적합 모델의 가중치를 피클 파일로 저장\n",
    "best_model_weights = history.model.get_weights()\n",
    "pickle.dump(best_model_weights, open('../model/best_model_weights_cnn.pkl', 'wb'))\n",
    "\n",
    "# 레이블 인코더의 클래스 정보를 저장\n",
    "np.save('../model/label_encoder_classes_cnn.npy', label_encoder.classes_)\n",
    "\n",
    "# 가중치를 로드하기 위해 모델 구성\n",
    "loaded_model = keras.models.Sequential()\n",
    "loaded_model.add(keras.layers.LSTM(128, input_shape=(3, 4)))  # 3개의 시퀀스, 각 시퀀스에 4개의 피처\n",
    "loaded_model.add(keras.layers.Dense(5, activation='softmax'))  # 분류할 클래스 수에 맞게 조정\n",
    "\n",
    "# 피클 파일에서 가중치 로드\n",
    "loaded_model_weights = pickle.load(open('../model/best_model_weights_cnn.pkl', 'rb'))\n",
    "\n",
    "# 모델에 로드된 가중치 설정\n",
    "loaded_model.set_weights(loaded_model_weights)\n",
    "\n",
    "# 모델 컴파일\n",
    "loaded_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 모델 저장\n",
    "loaded_model.save('../model/loaded_model_cnn.h5')\n",
    "\n",
    "# 예측할 데이터 불러오기\n",
    "pred_df = pd.read_csv('../data/Arduino/fall_sc_cnn.csv')\n",
    "\n",
    "# 입력 데이터 준비\n",
    "X_pred = []  # 입력 데이터\n",
    "\n",
    "# 4개의 레코드씩 묶어서 처리\n",
    "for i in range(0, len(pred_df) - 3, 4):\n",
    "    # gx, gy, gz 값 가져오기\n",
    "    gx_values = pred_df.loc[i:i+3, 'scaled_gx'].values\n",
    "    gy_values = pred_df.loc[i:i+3, 'scaled_gy'].values\n",
    "    gz_values = pred_df.loc[i:i+3, 'scaled_gz'].values\n",
    "\n",
    "    # 데이터 형상 조정\n",
    "    gx_values = np.transpose(gx_values)\n",
    "    gy_values = np.transpose(gy_values)\n",
    "    gz_values = np.transpose(gz_values)\n",
    "\n",
    "    # 시퀀스로 변환하여 입력 데이터에 추가\n",
    "    X_pred.append([gx_values, gy_values, gz_values])\n",
    "\n",
    "# 입력 데이터를 넘파이 배열로 변환\n",
    "X_pred = np.array(X_pred)\n",
    "\n",
    "# 모델 로드\n",
    "try:\n",
    "    best_model = keras.models.load_model('../model/loaded_model_cnn.h5')\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"모델 파일을 찾을 수 없습니다: {e}\")\n",
    "    exit()\n",
    "\n",
    "# 모델을 사용하여 예측\n",
    "predictions = best_model.predict(X_pred)\n",
    "\n",
    "# 예측 결과 디코딩\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.classes_ = np.load('../model/label_encoder_classes_cnn.npy')\n",
    "decoded_predictions = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
    "\n",
    "# 예측 결과를 데이터프레임에 추가\n",
    "pred_df['predicted_label_cnn'] = np.repeat(decoded_predictions, 4)\n",
    "\n",
    "# 예측 결과를 CSV 파일로 저장\n",
    "pred_df.to_csv('../data/prediction_results_cnn.csv', index=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-06-10T03:38:05.913885600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import pickle\n",
    "\n",
    "# 데이터셋 준비\n",
    "train_df = pd.read_csv('../data/SisFall_train.csv')  # 훈련 파일\n",
    "test_df = pd.read_csv('../data/SisFall_test.csv')  # 테스트 파일\n",
    "\n",
    "X_train = []  # 훈련 입력 데이터\n",
    "y_train = []  # 훈련 출력 데이터\n",
    "\n",
    "X_test = []  # 테스트 입력 데이터\n",
    "y_test = []  # 테스트 출력 데이터\n",
    "\n",
    "# 4개의 레코드씩 묶어서 처리\n",
    "for i in range(0, len(train_df) - 3, 4):\n",
    "    # gx, gy, gz 값 가져오기\n",
    "    gx_values = train_df.loc[i:i+3, 'scaled_gx'].values\n",
    "    gy_values = train_df.loc[i:i+3, 'scaled_gy'].values\n",
    "    gz_values = train_df.loc[i:i+3, 'scaled_gz'].values\n",
    "\n",
    "    # 레이블 값 가져오기\n",
    "    labels = train_df.loc[i:i+3, 'label'].values\n",
    "\n",
    "    # 서로 다른 레이블이 포함된 경우 해당 시퀀스는 분석에서 제외\n",
    "    if len(set(labels)) == 1:\n",
    "        # 시퀀스로 변환하여 입력 데이터에 추가\n",
    "        X_train.append([gx_values, gy_values, gz_values])\n",
    "\n",
    "        # 레이블 값 중복 제거하여 출력 데이터에 추가\n",
    "        y_train.append(labels[0])\n",
    "\n",
    "for i in range(0, len(test_df) - 3, 4):\n",
    "    # gx, gy, gz 값 가져오기\n",
    "    gx_values = test_df.loc[i:i+3, 'scaled_gx'].values\n",
    "    gy_values = test_df.loc[i:i+3, 'scaled_gy'].values\n",
    "    gz_values = test_df.loc[i:i+3, 'scaled_gz'].values\n",
    "\n",
    "    # 레이블 값 가져오기\n",
    "    labels = test_df.loc[i:i+3, 'label'].values\n",
    "\n",
    "    # 서로 다른 레이블이 포함된 경우 해당 시퀀스는 분석에서 제외\n",
    "    if len(set(labels)) == 1:\n",
    "        # 시퀀스로 변환하여 입력 데이터에 추가\n",
    "        X_test.append([gx_values, gy_values, gz_values])\n",
    "\n",
    "        # 레이블 값 중복 제거하여 출력 데이터에 추가\n",
    "        y_test.append(labels[0])\n",
    "\n",
    "# 입력 데이터와 출력 데이터를 넘파이 배열로 변환\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# 출력 데이터를 One-Hot 인코딩\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded_train = label_encoder.fit_transform(y_train)\n",
    "integer_encoded_train = integer_encoded_train.reshape(len(integer_encoded_train), 1)\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "y_train = onehot_encoder.fit_transform(integer_encoded_train)\n",
    "\n",
    "integer_encoded_test = label_encoder.transform(y_test)\n",
    "integer_encoded_test = integer_encoded_test.reshape(len(integer_encoded_test), 1)\n",
    "y_test = onehot_encoder.transform(integer_encoded_test)\n",
    "\n",
    "# LSTM 모델 정의\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.LSTM(128, input_shape=(3, 4)))  # 3개의 시퀀스, 각 시퀀스에 4개의 피처\n",
    "model.add(keras.layers.Dense(5, activation='softmax'))  # 분류할 클래스 수에 맞게 조정\n",
    "\n",
    "# 모델 컴파일 및 학습\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# 최상의 적합 모델의 가중치를 피클 파일로 저장\n",
    "best_model_weights = history.model.get_weights()\n",
    "pickle.dump(best_model_weights, open('../model/best_model_weights_cnn.pkl', 'wb'))\n",
    "\n",
    "# 레이블 인코더의 클래스 정보를 저장\n",
    "np.save('../model/label_encoder_classes_cnn.npy', label_encoder.classes_)\n",
    "\n",
    "# 가중치를 로드하기 위해 모델 구성\n",
    "loaded_model = keras.models.Sequential()\n",
    "loaded_model.add(keras.layers.LSTM(128, input_shape=(3, 4)))  # 3개의 시퀀스, 각 시퀀스에 4개의 피처\n",
    "loaded_model.add(keras.layers.Dense(5, activation='softmax'))  # 분류할 클래스 수에 맞게 조정\n",
    "\n",
    "# 피클 파일에서 가중치 로드\n",
    "loaded_model_weights = pickle.load(open('../model/best_model_weights_cnn.pkl', 'rb'))\n",
    "\n",
    "# 모델에 로드된 가중치 설정\n",
    "loaded_model.set_weights(loaded_model_weights)\n",
    "\n",
    "# 모델 컴파일\n",
    "loaded_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 모델 저장\n",
    "loaded_model.save('../model/loaded_model_cnn.h5')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "# 예측할 데이터 불러오기\n",
    "pred_df = pd.read_csv('../data/Arduino/fall_sc.csv')\n",
    "\n",
    "# 입력 데이터 준비\n",
    "X_pred = []  # 입력 데이터\n",
    "\n",
    "# 4개의 레코드씩 묶어서 처리\n",
    "for i in range(0, len(pred_df) - 3, 4):\n",
    "    # gx, gy, gz 값 가져오기\n",
    "    gx_values = pred_df.loc[i:i+3, 'scaled_gx'].values\n",
    "    gy_values = pred_df.loc[i:i+3, 'scaled_gy'].values\n",
    "    gz_values = pred_df.loc[i:i+3, 'scaled_gz'].values\n",
    "\n",
    "    # 데이터 형상 조정\n",
    "    gx_values = np.transpose(gx_values)\n",
    "    gy_values = np.transpose(gy_values)\n",
    "    gz_values = np.transpose(gz_values)\n",
    "\n",
    "    # 시퀀스로 변환하여 입력 데이터에 추가\n",
    "    X_pred.append([gx_values, gy_values, gz_values])\n",
    "\n",
    "# 입력 데이터를 넘파이 배열로 변환\n",
    "X_pred = np.array(X_pred)\n",
    "\n",
    "# 예측\n",
    "predictions = loaded_model.predict(X_pred)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "predicted_classes = label_encoder.inverse_transform(predicted_labels)\n",
    "\n",
    "# 예측 결과 출력\n",
    "print(predicted_classes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-09T03:21:29.811809500Z",
     "start_time": "2023-06-09T03:21:28.817832700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import pickle\n",
    "\n",
    "# 데이터셋 준비\n",
    "train_df = pd.read_csv('../data/SisFall_train.csv')  # 훈련 파일\n",
    "test_df = pd.read_csv('../data/SisFall_test.csv')  # 테스트 파일\n",
    "\n",
    "X_train = []  # 훈련 입력 데이터\n",
    "y_train = []  # 훈련 출력 데이터\n",
    "\n",
    "X_test = []  # 테스트 입력 데이터\n",
    "y_test = []  # 테스트 출력 데이터\n",
    "\n",
    "# 4개의 레코드씩 묶어서 처리\n",
    "for i in range(0, len(train_df) - 39, 40):\n",
    "    # gx, gy, gz 값 가져오기\n",
    "    gx_values = train_df.loc[i:i+39, 'scaled_gx'].values\n",
    "    gy_values = train_df.loc[i:i+39, 'scaled_gy'].values\n",
    "    gz_values = train_df.loc[i:i+39, 'scaled_gz'].values\n",
    "\n",
    "    # 레이블 값 가져오기\n",
    "    labels = train_df.loc[i:i+39, 'label'].values\n",
    "\n",
    "    # 서로 다른 레이블이 포함된 경우 해당 시퀀스는 분석에서 제외\n",
    "    if len(set(labels)) == 1:\n",
    "        # 시퀀스로 변환하여 입력 데이터에 추가\n",
    "        X_train.append([gx_values, gy_values, gz_values])\n",
    "\n",
    "        # 레이블 값 중복 제거하여 출력 데이터에 추가\n",
    "        y_train.append(labels[0])\n",
    "\n",
    "for i in range(0, len(test_df) - 39, 40):\n",
    "    # gx, gy, gz 값 가져오기\n",
    "    gx_values = test_df.loc[i:i+39, 'scaled_gx'].values\n",
    "    gy_values = test_df.loc[i:i+39, 'scaled_gy'].values\n",
    "    gz_values = test_df.loc[i:i+39, 'scaled_gz'].values\n",
    "\n",
    "    # 레이블 값 가져오기\n",
    "    labels = test_df.loc[i:i+39, 'label'].values\n",
    "\n",
    "    # 서로 다른 레이블이 포함된 경우 해당 시퀀스는 분석에서 제외\n",
    "    if len(set(labels)) == 1:\n",
    "        # 시퀀스로 변환하여 입력 데이터에 추가\n",
    "        X_test.append([gx_values, gy_values, gz_values])\n",
    "\n",
    "        # 레이블 값 중복 제거하여 출력 데이터에 추가\n",
    "        y_test.append(labels[0])\n",
    "\n",
    "# 입력 데이터와 출력 데이터를 넘파이 배열로 변환\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# 출력 데이터를 One-Hot 인코딩\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded_train = label_encoder.fit_transform(y_train)\n",
    "integer_encoded_train = integer_encoded_train.reshape(len(integer_encoded_train), 1)\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "y_train = onehot_encoder.fit_transform(integer_encoded_train)\n",
    "\n",
    "integer_encoded_test = label_encoder.transform(y_test)\n",
    "integer_encoded_test = integer_encoded_test.reshape(len(integer_encoded_test), 1)\n",
    "y_test = onehot_encoder.transform(integer_encoded_test)\n",
    "\n",
    "# LSTM 모델 정의\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.LSTM(128, input_shape=(3, 40)))  # 3개의 시퀀스, 각 시퀀스에 40개의 피처\n",
    "model.add(keras.layers.Dense(5, activation='softmax'))  # 분류할 클래스 수에 맞게 조정\n",
    "\n",
    "# 모델 컴파일 및 학습\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# 최상의 적합 모델의 가중치를 피클 파일로 저장\n",
    "best_model_weights = history.model.get_weights()\n",
    "pickle.dump(best_model_weights, open('../model/best_model_weights_cnn.pkl', 'wb'))\n",
    "\n",
    "# 레이블 인코더의 클래스 정보를 저장\n",
    "np.save('../model/label_encoder_classes_cnn.npy', label_encoder.classes_)\n",
    "\n",
    "# 가중치를 로드하기 위해 모델 구성\n",
    "loaded_model = keras.models.Sequential()\n",
    "loaded_model.add(keras.layers.LSTM(128, input_shape=(3, 40)))  # 3개의 시퀀스, 각 시퀀스에 40개의 피처\n",
    "loaded_model.add(keras.layers.Dense(5, activation='softmax'))  # 분류할 클래스 수에 맞게 조정\n",
    "\n",
    "# 피클 파일에서 가중치 로드\n",
    "loaded_model_weights = pickle.load(open('../model/best_model_weights_cnn.pkl', 'rb'))\n",
    "\n",
    "# 모델에 로드된 가중치 설정\n",
    "loaded_model.set_weights(loaded_model_weights)\n",
    "\n",
    "# 모델 컴파일\n",
    "loaded_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 모델 저장\n",
    "loaded_model.save('../model/loaded_model_cnn.h5')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# 데이터셋 준비\n",
    "X = []  # 입력 데이터\n",
    "y = []  # 출력 데이터\n",
    "\n",
    "# 4개의 레코드씩 묶어서 처리\n",
    "for i in range(0, len(df) - 3, 4):\n",
    "    # gx, gy, gz 값 가져오기\n",
    "    gx_values = df.loc[i:i+3, 'scaled_gx'].values\n",
    "    gy_values = df.loc[i:i+3, 'scaled_gy'].values\n",
    "    gz_values = df.loc[i:i+3, 'scaled_gz'].values\n",
    "\n",
    "    # 레이블 값 가져오기\n",
    "    labels = df.loc[i:i+3, 'label'].values\n",
    "\n",
    "    # 서로 다른 레이블이 포함된 경우 해당 시퀀스는 분석에서 제외\n",
    "    if len(set(labels)) == 1:\n",
    "        # 시퀀스로 변환하여 입력 데이터에 추가\n",
    "        X.append([gx_values, gy_values, gz_values])\n",
    "\n",
    "        # 레이블 값 중복 제거하여 출력 데이터에 추가\n",
    "        y.append(labels[0])\n",
    "\n",
    "# 입력 데이터와 출력 데이터를 넘파이 배열로 변환\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# 출력 데이터를 One-Hot 인코딩\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(y)\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "y = onehot_encoder.fit_transform(integer_encoded)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-09T03:31:31.164647200Z",
     "start_time": "2023-06-09T03:31:14.070384300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2590/2590 [==============================] - 7s 2ms/step - loss: 1.5233 - accuracy: 0.3102\n",
      "Epoch 2/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 1.2342 - accuracy: 0.4656\n",
      "Epoch 3/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 1.1547 - accuracy: 0.5015\n",
      "Epoch 4/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 1.0542 - accuracy: 0.5506\n",
      "Epoch 5/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.9962 - accuracy: 0.5817\n",
      "Epoch 6/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.9684 - accuracy: 0.5953\n",
      "Epoch 7/100\n",
      "2590/2590 [==============================] - 6s 2ms/step - loss: 0.9537 - accuracy: 0.6011\n",
      "Epoch 8/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.9451 - accuracy: 0.6066\n",
      "Epoch 9/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.9372 - accuracy: 0.6092\n",
      "Epoch 10/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.9307 - accuracy: 0.6107\n",
      "Epoch 11/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.9243 - accuracy: 0.6144\n",
      "Epoch 12/100\n",
      "2590/2590 [==============================] - 6s 2ms/step - loss: 0.9194 - accuracy: 0.6170\n",
      "Epoch 13/100\n",
      "2590/2590 [==============================] - 6s 2ms/step - loss: 0.9142 - accuracy: 0.6196\n",
      "Epoch 14/100\n",
      "2590/2590 [==============================] - 6s 2ms/step - loss: 0.9083 - accuracy: 0.6217\n",
      "Epoch 15/100\n",
      "2590/2590 [==============================] - 6s 2ms/step - loss: 0.9034 - accuracy: 0.6232\n",
      "Epoch 16/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.8985 - accuracy: 0.6250\n",
      "Epoch 17/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.8929 - accuracy: 0.6282\n",
      "Epoch 18/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.8871 - accuracy: 0.6291\n",
      "Epoch 19/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.8820 - accuracy: 0.6314\n",
      "Epoch 20/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.8767 - accuracy: 0.6340\n",
      "Epoch 21/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.8720 - accuracy: 0.6360\n",
      "Epoch 22/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.8676 - accuracy: 0.6373\n",
      "Epoch 23/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.8608 - accuracy: 0.6400\n",
      "Epoch 24/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.8550 - accuracy: 0.6420\n",
      "Epoch 25/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.8500 - accuracy: 0.6448\n",
      "Epoch 26/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.8441 - accuracy: 0.6457\n",
      "Epoch 27/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.8398 - accuracy: 0.6481\n",
      "Epoch 28/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.8337 - accuracy: 0.6522\n",
      "Epoch 29/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.8300 - accuracy: 0.6520\n",
      "Epoch 30/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.8271 - accuracy: 0.6534\n",
      "Epoch 31/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.8221 - accuracy: 0.6561\n",
      "Epoch 32/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.8176 - accuracy: 0.6578\n",
      "Epoch 33/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.8142 - accuracy: 0.6596\n",
      "Epoch 34/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.8112 - accuracy: 0.6616\n",
      "Epoch 35/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.8064 - accuracy: 0.6621\n",
      "Epoch 36/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.8035 - accuracy: 0.6638\n",
      "Epoch 37/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.8004 - accuracy: 0.6643\n",
      "Epoch 38/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.7966 - accuracy: 0.6663\n",
      "Epoch 39/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.7944 - accuracy: 0.6668\n",
      "Epoch 40/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.7906 - accuracy: 0.6664\n",
      "Epoch 41/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.7872 - accuracy: 0.6684\n",
      "Epoch 42/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.7833 - accuracy: 0.6701\n",
      "Epoch 43/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.7809 - accuracy: 0.6716\n",
      "Epoch 44/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.7780 - accuracy: 0.6729\n",
      "Epoch 45/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.7745 - accuracy: 0.6738\n",
      "Epoch 46/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.7718 - accuracy: 0.6760\n",
      "Epoch 47/100\n",
      "2590/2590 [==============================] - 6s 2ms/step - loss: 0.7683 - accuracy: 0.6779\n",
      "Epoch 48/100\n",
      "2590/2590 [==============================] - 6s 2ms/step - loss: 0.7658 - accuracy: 0.6777\n",
      "Epoch 49/100\n",
      "2590/2590 [==============================] - 6s 2ms/step - loss: 0.7618 - accuracy: 0.6791\n",
      "Epoch 50/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.7602 - accuracy: 0.6798\n",
      "Epoch 51/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.7569 - accuracy: 0.6799\n",
      "Epoch 52/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.7537 - accuracy: 0.6835\n",
      "Epoch 53/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.7505 - accuracy: 0.6845\n",
      "Epoch 54/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.7483 - accuracy: 0.6860\n",
      "Epoch 55/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.7449 - accuracy: 0.6870\n",
      "Epoch 56/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.7424 - accuracy: 0.6878\n",
      "Epoch 57/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.7398 - accuracy: 0.6887\n",
      "Epoch 58/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.7370 - accuracy: 0.6894\n",
      "Epoch 59/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.7338 - accuracy: 0.6911\n",
      "Epoch 60/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.7314 - accuracy: 0.6927\n",
      "Epoch 61/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.7288 - accuracy: 0.6938\n",
      "Epoch 62/100\n",
      "2590/2590 [==============================] - 6s 2ms/step - loss: 0.7262 - accuracy: 0.6945\n",
      "Epoch 63/100\n",
      "2590/2590 [==============================] - 6s 2ms/step - loss: 0.7241 - accuracy: 0.6934\n",
      "Epoch 64/100\n",
      "2590/2590 [==============================] - 6s 2ms/step - loss: 0.7211 - accuracy: 0.6956\n",
      "Epoch 65/100\n",
      "2590/2590 [==============================] - 6s 2ms/step - loss: 0.7189 - accuracy: 0.6980\n",
      "Epoch 66/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.7167 - accuracy: 0.6996\n",
      "Epoch 67/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.7144 - accuracy: 0.6997\n",
      "Epoch 68/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.7108 - accuracy: 0.6996\n",
      "Epoch 69/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.7084 - accuracy: 0.7030\n",
      "Epoch 70/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.7063 - accuracy: 0.7034\n",
      "Epoch 71/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.7042 - accuracy: 0.7034\n",
      "Epoch 72/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.7012 - accuracy: 0.7054\n",
      "Epoch 73/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.7002 - accuracy: 0.7055\n",
      "Epoch 74/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.6970 - accuracy: 0.7070\n",
      "Epoch 75/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.6951 - accuracy: 0.7081\n",
      "Epoch 76/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.6926 - accuracy: 0.7090\n",
      "Epoch 77/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.6903 - accuracy: 0.7092\n",
      "Epoch 78/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.6874 - accuracy: 0.7116\n",
      "Epoch 79/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.6849 - accuracy: 0.7115\n",
      "Epoch 80/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.6841 - accuracy: 0.7118\n",
      "Epoch 81/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.6818 - accuracy: 0.7132\n",
      "Epoch 82/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.6786 - accuracy: 0.7145\n",
      "Epoch 83/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.6764 - accuracy: 0.7162\n",
      "Epoch 84/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.6757 - accuracy: 0.7161\n",
      "Epoch 85/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.6723 - accuracy: 0.7171\n",
      "Epoch 86/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.6707 - accuracy: 0.7173\n",
      "Epoch 87/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.6684 - accuracy: 0.7198\n",
      "Epoch 88/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.6651 - accuracy: 0.7203\n",
      "Epoch 89/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.6635 - accuracy: 0.7207\n",
      "Epoch 90/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.6624 - accuracy: 0.7230\n",
      "Epoch 91/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.6604 - accuracy: 0.7223\n",
      "Epoch 92/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.6572 - accuracy: 0.7252\n",
      "Epoch 93/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.6560 - accuracy: 0.7238\n",
      "Epoch 94/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.6543 - accuracy: 0.7256\n",
      "Epoch 95/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.6514 - accuracy: 0.7264\n",
      "Epoch 96/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.6498 - accuracy: 0.7279\n",
      "Epoch 97/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.6474 - accuracy: 0.7297\n",
      "Epoch 98/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.6460 - accuracy: 0.7290\n",
      "Epoch 99/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.6443 - accuracy: 0.7301\n",
      "Epoch 100/100\n",
      "2590/2590 [==============================] - 5s 2ms/step - loss: 0.6417 - accuracy: 0.7309\n"
     ]
    }
   ],
   "source": [
    "# LSTM 모델 정의\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.LSTM(128, input_shape=(3, 4)))  # 3개의 시퀀스, 각 시퀀스에 4개의 피처\n",
    "model.add(keras.layers.Dense(5, activation='softmax'))  # 분류할 클래스 수에 맞게 조정\n",
    "\n",
    "# 모델 컴파일 및 학습\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X, y, epochs=100, batch_size=32)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-09T03:40:25.245956400Z",
     "start_time": "2023-06-09T03:31:34.655313300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# 최상의 적합 모델의 가중치를 피클 파일로 저장\n",
    "best_model_weights = history.model.get_weights()\n",
    "pickle.dump(best_model_weights, open('../model/best_model_weights.pkl', 'wb'))\n",
    "\n",
    "# 레이블 인코더의 클래스 정보를 저장\n",
    "np.save('../model/label_encoder_classes.npy', label_encoder.classes_)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-09T03:41:22.183439Z",
     "start_time": "2023-06-09T03:41:22.167798Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 0s/step\n",
      "['fall' 'fall' 'stairs_walking' 'stairs_walking' 'stairs_walking' 'fall'\n",
      " 'jogging' 'fall' 'stairs_walking' 'fall' 'jogging' 'stairs_walking'\n",
      " 'stairs_walking' 'fall' 'fall' 'fall' 'stairs_walking' 'stairs_walking'\n",
      " 'fall' 'stairs_walking' 'fall' 'fall' 'stairs_walking' 'fall' 'fall'\n",
      " 'fall' 'fall' 'fall' 'stairs_walking' 'stairs_walking' 'stairs_walking'\n",
      " 'fall' 'stairs_walking' 'fall' 'jogging' 'jogging' 'fall'\n",
      " 'stairs_walking' 'stairs_walking' 'jogging' 'stairs_walking' 'fall'\n",
      " 'fall' 'fall' 'fall' 'stairs_walking' 'fall' 'jogging' 'fall'\n",
      " 'stairs_walking']\n"
     ]
    }
   ],
   "source": [
    "# 가중치를 로드하기 위해 모델 구성\n",
    "loaded_model = keras.models.Sequential()\n",
    "loaded_model.add(keras.layers.LSTM(128, input_shape=(3, 4)))  # 3개의 시퀀스, 각 시퀀스에 4개의 피처\n",
    "loaded_model.add(keras.layers.Dense(5, activation='softmax'))  # 분류할 클래스 수에 맞게 조정\n",
    "\n",
    "# 피클 파일에서 가중치 로드\n",
    "loaded_model_weights = pickle.load(open('../model/best_model_weights.pkl', 'rb'))\n",
    "\n",
    "# 모델에 로드된 가중치 설정\n",
    "loaded_model.set_weights(loaded_model_weights)\n",
    "\n",
    "# 모델 컴파일\n",
    "loaded_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 모델 저장\n",
    "loaded_model.save('../model/loaded_model_cnn.h5')\n",
    "\n",
    "# 예측할 데이터 불러오기\n",
    "pred_df = pd.read_csv('../data/Arduino/fall_sc.csv')\n",
    "\n",
    "# 입력 데이터 준비\n",
    "X_pred = []  # 입력 데이터\n",
    "\n",
    "# 4개의 레코드씩 묶어서 처리\n",
    "for i in range(0, len(pred_df) - 3, 4):\n",
    "    # gx, gy, gz 값 가져오기\n",
    "    gx_values = pred_df.loc[i:i+3, 'scaled_gx'].values\n",
    "    gy_values = pred_df.loc[i:i+3, 'scaled_gy'].values\n",
    "    gz_values = pred_df.loc[i:i+3, 'scaled_gz'].values\n",
    "\n",
    "    # 데이터 형상 조정\n",
    "    gx_values = np.transpose(gx_values)\n",
    "    gy_values = np.transpose(gy_values)\n",
    "    gz_values = np.transpose(gz_values)\n",
    "\n",
    "    # 시퀀스로 변환하여 입력 데이터에 추가\n",
    "    X_pred.append([gx_values, gy_values, gz_values])\n",
    "\n",
    "# 입력 데이터를 넘파이 배열로 변환\n",
    "X_pred = np.array(X_pred)\n",
    "\n",
    "# 예측\n",
    "predictions = loaded_model.predict(X_pred)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "predicted_classes = label_encoder.inverse_transform(predicted_labels)\n",
    "\n",
    "# 예측 결과 출력\n",
    "print(predicted_classes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-09T03:51:46.039572400Z",
     "start_time": "2023-06-09T03:51:44.832661500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "# 데이터셋 준비\n",
    "X = []  # 입력 데이터\n",
    "y = []  # 출력 데이터\n",
    "\n",
    "# 40개의 레코드씩 묶어서 처리\n",
    "for i in range(0, len(df) - 39, 40):\n",
    "    # gx, gy, gz 값 가져오기\n",
    "    gx_values = df.loc[i:i+39, 'scaled_gx'].values\n",
    "    gy_values = df.loc[i:i+39, 'scaled_gy'].values\n",
    "    gz_values = df.loc[i:i+39, 'scaled_gz'].values\n",
    "\n",
    "    # 레이블 값 가져오기\n",
    "    labels = df.loc[i:i+39, 'label'].values\n",
    "\n",
    "    # 서로 다른 레이블이 포함된 경우 해당 시퀀스는 분석에서 제외\n",
    "    if len(set(labels)) == 1:\n",
    "        # 시퀀스로 변환하여 입력 데이터에 추가\n",
    "        X.append([gx_values, gy_values, gz_values])\n",
    "\n",
    "        # 레이블 값 중복 제거하여 출력 데이터에 추가\n",
    "        y.append(labels[0])\n",
    "\n",
    "# 입력 데이터와 출력 데이터를 넘파이 배열로 변환\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# 출력 데이터를 One-Hot 인코딩\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(y)\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "y = onehot_encoder.fit_transform(integer_encoded)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-09T07:57:32.395929Z",
     "start_time": "2023-06-09T07:57:30.469997300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "256/256 [==============================] - 2s 2ms/step - loss: 1.5553 - accuracy: 0.2791\n",
      "Epoch 2/100\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 1.5483 - accuracy: 0.2915\n",
      "Epoch 3/100\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 1.5023 - accuracy: 0.3530\n",
      "Epoch 4/100\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 1.2800 - accuracy: 0.4432\n",
      "Epoch 5/100\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 1.1741 - accuracy: 0.4741\n",
      "Epoch 6/100\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 1.1233 - accuracy: 0.4895\n",
      "Epoch 7/100\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 1.0679 - accuracy: 0.5166\n",
      "Epoch 8/100\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 1.0090 - accuracy: 0.5469\n",
      "Epoch 9/100\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 0.9892 - accuracy: 0.5547\n",
      "Epoch 10/100\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 0.9277 - accuracy: 0.5818\n",
      "Epoch 11/100\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 0.9090 - accuracy: 0.5905\n",
      "Epoch 12/100\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 0.8940 - accuracy: 0.5936\n",
      "Epoch 13/100\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 0.8739 - accuracy: 0.6046\n",
      "Epoch 14/100\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 0.8624 - accuracy: 0.6069\n",
      "Epoch 15/100\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 0.8422 - accuracy: 0.6222\n",
      "Epoch 16/100\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 0.8440 - accuracy: 0.6257\n",
      "Epoch 17/100\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 0.8271 - accuracy: 0.6236\n",
      "Epoch 18/100\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 0.8142 - accuracy: 0.6353\n",
      "Epoch 19/100\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 0.8136 - accuracy: 0.6360\n",
      "Epoch 20/100\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 0.7962 - accuracy: 0.6467\n",
      "Epoch 21/100\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 0.7982 - accuracy: 0.6449\n",
      "Epoch 22/100\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 0.7879 - accuracy: 0.6491\n",
      "Epoch 23/100\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 0.7813 - accuracy: 0.6504\n",
      "Epoch 24/100\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 0.7749 - accuracy: 0.6608\n",
      "Epoch 25/100\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 0.7602 - accuracy: 0.6634\n",
      "Epoch 26/100\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 0.7528 - accuracy: 0.6696\n",
      "Epoch 27/100\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 0.7504 - accuracy: 0.6750\n",
      "Epoch 28/100\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 0.7400 - accuracy: 0.6760\n",
      "Epoch 29/100\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 0.7325 - accuracy: 0.6837\n",
      "Epoch 30/100\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 0.7193 - accuracy: 0.6907\n",
      "Epoch 31/100\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 0.7027 - accuracy: 0.6991\n",
      "Epoch 32/100\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 0.6933 - accuracy: 0.7070\n",
      "Epoch 33/100\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 0.6734 - accuracy: 0.7127\n",
      "Epoch 34/100\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 0.6596 - accuracy: 0.7231\n",
      "Epoch 35/100\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 0.6447 - accuracy: 0.7301\n",
      "Epoch 36/100\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 0.6371 - accuracy: 0.7404\n",
      "Epoch 37/100\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 0.6165 - accuracy: 0.7434\n",
      "Epoch 38/100\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 0.5984 - accuracy: 0.7535\n",
      "Epoch 39/100\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 0.5910 - accuracy: 0.7549\n",
      "Epoch 40/100\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 0.5738 - accuracy: 0.7671\n",
      "Epoch 41/100\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 0.5675 - accuracy: 0.7663\n",
      "Epoch 42/100\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 0.5609 - accuracy: 0.7679\n",
      "Epoch 43/100\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 0.5502 - accuracy: 0.7738\n",
      "Epoch 44/100\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 0.5426 - accuracy: 0.7759\n",
      "Epoch 45/100\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 0.5274 - accuracy: 0.7881\n",
      "Epoch 46/100\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 0.5264 - accuracy: 0.7825\n",
      "Epoch 47/100\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 0.5131 - accuracy: 0.7922\n",
      "Epoch 48/100\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 0.5445 - accuracy: 0.7780\n",
      "Epoch 49/100\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 0.4969 - accuracy: 0.7977\n",
      "Epoch 50/100\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 0.4949 - accuracy: 0.8021\n",
      "Epoch 51/100\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 0.4891 - accuracy: 0.8031\n",
      "Epoch 52/100\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 0.4844 - accuracy: 0.8017\n",
      "Epoch 53/100\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 0.4797 - accuracy: 0.8046\n",
      "Epoch 54/100\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 0.4750 - accuracy: 0.8054\n",
      "Epoch 55/100\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 0.4622 - accuracy: 0.8109\n",
      "Epoch 56/100\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 0.4601 - accuracy: 0.8122\n",
      "Epoch 57/100\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 0.4579 - accuracy: 0.8126\n",
      "Epoch 58/100\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 0.4506 - accuracy: 0.8150\n",
      "Epoch 59/100\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 0.4481 - accuracy: 0.8164\n",
      "Epoch 60/100\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 0.4451 - accuracy: 0.8206\n",
      "Epoch 61/100\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 0.4367 - accuracy: 0.8233\n",
      "Epoch 62/100\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 0.4307 - accuracy: 0.8240\n",
      "Epoch 63/100\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 0.4245 - accuracy: 0.8298\n",
      "Epoch 64/100\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 0.4247 - accuracy: 0.8292\n",
      "Epoch 65/100\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 0.4165 - accuracy: 0.8304\n",
      "Epoch 66/100\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 0.4124 - accuracy: 0.8378\n",
      "Epoch 67/100\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 0.4045 - accuracy: 0.8392\n",
      "Epoch 68/100\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 0.4031 - accuracy: 0.8342\n",
      "Epoch 69/100\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 0.4245 - accuracy: 0.8323\n",
      "Epoch 70/100\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 0.3945 - accuracy: 0.8429\n",
      "Epoch 71/100\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 0.3924 - accuracy: 0.8402\n",
      "Epoch 72/100\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 0.3869 - accuracy: 0.8443\n",
      "Epoch 73/100\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 0.3798 - accuracy: 0.8490\n",
      "Epoch 74/100\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 0.3730 - accuracy: 0.8482\n",
      "Epoch 75/100\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 0.3715 - accuracy: 0.8473\n",
      "Epoch 76/100\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 0.3765 - accuracy: 0.8467\n",
      "Epoch 77/100\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 0.3668 - accuracy: 0.8492\n",
      "Epoch 78/100\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 0.3579 - accuracy: 0.8561\n",
      "Epoch 79/100\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 0.3549 - accuracy: 0.8553\n",
      "Epoch 80/100\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 0.3542 - accuracy: 0.8532\n",
      "Epoch 81/100\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 0.3483 - accuracy: 0.8598\n",
      "Epoch 82/100\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 0.3454 - accuracy: 0.8590\n",
      "Epoch 83/100\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 0.3346 - accuracy: 0.8664\n",
      "Epoch 84/100\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 0.3356 - accuracy: 0.8634\n",
      "Epoch 85/100\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 0.3327 - accuracy: 0.8619\n",
      "Epoch 86/100\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 0.3366 - accuracy: 0.8633\n",
      "Epoch 87/100\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 0.3336 - accuracy: 0.8618\n",
      "Epoch 88/100\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 0.3163 - accuracy: 0.8694\n",
      "Epoch 89/100\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 0.3107 - accuracy: 0.8766\n",
      "Epoch 90/100\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 0.3137 - accuracy: 0.8716\n",
      "Epoch 91/100\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 0.3039 - accuracy: 0.8787\n",
      "Epoch 92/100\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 0.3056 - accuracy: 0.8775\n",
      "Epoch 93/100\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 0.2981 - accuracy: 0.8799\n",
      "Epoch 94/100\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 0.2964 - accuracy: 0.8803\n",
      "Epoch 95/100\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 0.2891 - accuracy: 0.8818\n",
      "Epoch 96/100\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 0.2901 - accuracy: 0.8827\n",
      "Epoch 97/100\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 0.2871 - accuracy: 0.8829\n",
      "Epoch 98/100\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 0.2760 - accuracy: 0.8892\n",
      "Epoch 99/100\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 0.2721 - accuracy: 0.8919\n",
      "Epoch 100/100\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 0.2802 - accuracy: 0.8885\n"
     ]
    }
   ],
   "source": [
    "# LSTM 모델 정의\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.LSTM(128, input_shape=(3, 40)))  # 40개의 시퀀스, 각 시퀀스에 3개의 피처\n",
    "model.add(keras.layers.Dense(5, activation='softmax'))  # 분류할 클래스 수에 맞게 조정\n",
    "\n",
    "# 모델 컴파일 및 학습\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X, y, epochs=100, batch_size=32)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-09T07:59:13.035655600Z",
     "start_time": "2023-06-09T07:58:19.867069600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "# 최상의 적합 모델의 가중치를 피클 파일로 저장\n",
    "best_model_weights = history.model.get_weights()\n",
    "pickle.dump(best_model_weights, open('../model/best_model_weights.pkl', 'wb'))\n",
    "\n",
    "# 레이블 인코더의 클래스 정보를 저장\n",
    "np.save('../model/label_encoder_classes.npy', label_encoder.classes_)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-09T07:59:55.858370700Z",
     "start_time": "2023-06-09T07:59:55.845231100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 435ms/step\n",
      "['stairs_walking' 'fall' 'stairs_walking' 'jogging' 'fall']\n"
     ]
    }
   ],
   "source": [
    "# 가중치를 로드하기 위해 모델 구성\n",
    "loaded_model = keras.models.Sequential()\n",
    "loaded_model.add(keras.layers.LSTM(128, input_shape=(3, 40)))  # 3개의 시퀀스, 각 시퀀스에 4개의 피처\n",
    "loaded_model.add(keras.layers.Dense(5, activation='softmax'))  # 분류할 클래스 수에 맞게 조정\n",
    "\n",
    "# 피클 파일에서 가중치 로드\n",
    "loaded_model_weights = pickle.load(open('../model/best_model_weights.pkl', 'rb'))\n",
    "\n",
    "# 모델에 로드된 가중치 설정\n",
    "loaded_model.set_weights(loaded_model_weights)\n",
    "\n",
    "# 모델 컴파일\n",
    "loaded_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 모델 저장\n",
    "loaded_model.save('../model/loaded_model_cnn.h5')\n",
    "\n",
    "# 예측할 데이터 불러오기\n",
    "pred_df = pd.read_csv('../data/Arduino/fall_sc.csv')\n",
    "\n",
    "# 입력 데이터 준비\n",
    "X_pred = []  # 입력 데이터\n",
    "\n",
    "# 4개의 레코드씩 묶어서 처리\n",
    "for i in range(0, len(pred_df) - 3, 40):\n",
    "    # gx, gy, gz 값 가져오기\n",
    "    gx_values = pred_df.loc[i:i+39, 'scaled_gx'].values\n",
    "    gy_values = pred_df.loc[i:i+39, 'scaled_gy'].values\n",
    "    gz_values = pred_df.loc[i:i+39, 'scaled_gz'].values\n",
    "\n",
    "    # 데이터 형상 조정\n",
    "    gx_values = np.transpose(gx_values)\n",
    "    gy_values = np.transpose(gy_values)\n",
    "    gz_values = np.transpose(gz_values)\n",
    "\n",
    "    # 시퀀스로 변환하여 입력 데이터에 추가\n",
    "    X_pred.append([gx_values, gy_values, gz_values])\n",
    "\n",
    "# 입력 데이터를 넘파이 배열로 변환\n",
    "X_pred = np.array(X_pred)\n",
    "\n",
    "# 예측\n",
    "predictions = loaded_model.predict(X_pred)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "predicted_classes = label_encoder.inverse_transform(predicted_labels)\n",
    "\n",
    "# 예측 결과 출력\n",
    "print(predicted_classes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-09T08:01:38.086849300Z",
     "start_time": "2023-06-09T08:01:37.196403400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 8 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001DCF23816C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 407ms/step\n",
      "['jogging' 'jogging' 'jogging' 'fall' 'jogging' 'fall' 'jogging' 'fall'\n",
      " 'jogging' 'jogging']\n"
     ]
    }
   ],
   "source": [
    "# 가중치를 로드하기 위해 모델 구성\n",
    "loaded_model = keras.models.Sequential()\n",
    "loaded_model.add(keras.layers.LSTM(128, input_shape=(3, 40)))  # 3개의 시퀀스, 각 시퀀스에 4개의 피처\n",
    "loaded_model.add(keras.layers.Dense(5, activation='softmax'))  # 분류할 클래스 수에 맞게 조정\n",
    "\n",
    "# 피클 파일에서 가중치 로드\n",
    "loaded_model_weights = pickle.load(open('../model/best_model_weights.pkl', 'rb'))\n",
    "\n",
    "# 모델에 로드된 가중치 설정\n",
    "loaded_model.set_weights(loaded_model_weights)\n",
    "\n",
    "# 모델 컴파일\n",
    "loaded_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 모델 저장\n",
    "loaded_model.save('../model/loaded_model_cnn.h5')\n",
    "\n",
    "# 예측할 데이터 불러오기\n",
    "pred_df = pd.read_csv('../data/Arduino/walking_sc.csv')\n",
    "\n",
    "# 입력 데이터 준비\n",
    "X_pred = []  # 입력 데이터\n",
    "\n",
    "# 4개의 레코드씩 묶어서 처리\n",
    "for i in range(0, len(pred_df) - 3, 40):\n",
    "    # gx, gy, gz 값 가져오기\n",
    "    gx_values = pred_df.loc[i:i+39, 'scaled_gx'].values\n",
    "    gy_values = pred_df.loc[i:i+39, 'scaled_gy'].values\n",
    "    gz_values = pred_df.loc[i:i+39, 'scaled_gz'].values\n",
    "\n",
    "    # 데이터 형상 조정\n",
    "    gx_values = np.transpose(gx_values)\n",
    "    gy_values = np.transpose(gy_values)\n",
    "    gz_values = np.transpose(gz_values)\n",
    "\n",
    "    # 시퀀스로 변환하여 입력 데이터에 추가\n",
    "    X_pred.append([gx_values, gy_values, gz_values])\n",
    "\n",
    "# 입력 데이터를 넘파이 배열로 변환\n",
    "X_pred = np.array(X_pred)\n",
    "\n",
    "# 예측\n",
    "predictions = loaded_model.predict(X_pred)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "predicted_classes = label_encoder.inverse_transform(predicted_labels)\n",
    "\n",
    "# 예측 결과 출력\n",
    "print(predicted_classes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-09T08:14:52.101514200Z",
     "start_time": "2023-06-09T08:14:51.121825900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001DCF2382B00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "['fall' 'fall' 'fall' 'jogging' 'jogging' 'jogging']\n"
     ]
    }
   ],
   "source": [
    "# 가중치를 로드하기 위해 모델 구성\n",
    "loaded_model = keras.models.Sequential()\n",
    "loaded_model.add(keras.layers.LSTM(128, input_shape=(3, 40)))  # 3개의 시퀀스, 각 시퀀스에 4개의 피처\n",
    "loaded_model.add(keras.layers.Dense(5, activation='softmax'))  # 분류할 클래스 수에 맞게 조정\n",
    "\n",
    "# 피클 파일에서 가중치 로드\n",
    "loaded_model_weights = pickle.load(open('../model/best_model_weights.pkl', 'rb'))\n",
    "\n",
    "# 모델에 로드된 가중치 설정\n",
    "loaded_model.set_weights(loaded_model_weights)\n",
    "\n",
    "# 모델 컴파일\n",
    "loaded_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 모델 저장\n",
    "loaded_model.save('../model/loaded_model_cnn.h5')\n",
    "\n",
    "# 예측할 데이터 불러오기\n",
    "pred_df = pd.read_csv('../data/Arduino/standing_sc.csv')\n",
    "\n",
    "# 입력 데이터 준비\n",
    "X_pred = []  # 입력 데이터\n",
    "\n",
    "# 4개의 레코드씩 묶어서 처리\n",
    "for i in range(0, len(pred_df) - 3, 40):\n",
    "    # gx, gy, gz 값 가져오기\n",
    "    gx_values = pred_df.loc[i:i+39, 'scaled_gx'].values\n",
    "    gy_values = pred_df.loc[i:i+39, 'scaled_gy'].values\n",
    "    gz_values = pred_df.loc[i:i+39, 'scaled_gz'].values\n",
    "\n",
    "    # 데이터 형상 조정\n",
    "    gx_values = np.transpose(gx_values)\n",
    "    gy_values = np.transpose(gy_values)\n",
    "    gz_values = np.transpose(gz_values)\n",
    "\n",
    "    # 시퀀스로 변환하여 입력 데이터에 추가\n",
    "    X_pred.append([gx_values, gy_values, gz_values])\n",
    "\n",
    "# 입력 데이터를 넘파이 배열로 변환\n",
    "X_pred = np.array(X_pred)\n",
    "\n",
    "# 예측\n",
    "predictions = loaded_model.predict(X_pred)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "predicted_classes = label_encoder.inverse_transform(predicted_labels)\n",
    "\n",
    "# 예측 결과 출력\n",
    "print(predicted_classes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-09T08:16:23.752973700Z",
     "start_time": "2023-06-09T08:16:21.806423700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
