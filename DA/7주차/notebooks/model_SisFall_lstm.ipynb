{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       user_ID trial_ID task_ID         gx         gy         gz   \n",
      "0         SA01      R01     D01  -1.098633 -30.761719 -21.484375  \\\n",
      "1         SA01      R01     D01 -10.864258 -46.752930  -3.173828   \n",
      "2         SA01      R01     D01  31.860352 -22.216797   8.056641   \n",
      "3         SA01      R01     D01   2.624512 -11.352539  29.052734   \n",
      "4         SA01      R01     D01   7.263184  15.869141  26.184082   \n",
      "...        ...      ...     ...        ...        ...        ...   \n",
      "300097    SE14      R06     D07  -3.479004   2.563477  -0.061035   \n",
      "300098    SE14      R06     D07  -2.197266   3.234863   0.488281   \n",
      "300099    SE14      R06     D07  -4.394531   2.990723   0.549316   \n",
      "300100    SE14      R06     D07  -2.746582   2.563477   0.000000   \n",
      "300101    SE14      R06     D07  -4.028320   2.258301   0.122070   \n",
      "\n",
      "                   label  scaled_gx  scaled_gy  scaled_gz  \n",
      "0                walking  -0.284779  -0.036755   0.042287  \n",
      "1                walking  -0.291049  -0.056733   0.066825  \n",
      "2                walking  -0.263618  -0.026079   0.081875  \n",
      "3                walking  -0.282389  -0.012506   0.110011  \n",
      "4                walking  -0.279411   0.021504   0.106167  \n",
      "...                  ...        ...        ...        ...  \n",
      "300097  standing-sitting  -0.286308   0.004880   0.070996  \n",
      "300098  standing-sitting  -0.285485   0.005719   0.071732  \n",
      "300099  standing-sitting  -0.286896   0.005414   0.071814  \n",
      "300100  standing-sitting  -0.285837   0.004880   0.071078  \n",
      "300101  standing-sitting  -0.286660   0.004499   0.071242  \n",
      "\n",
      "[300102 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data/SisFall_train.csv')\n",
    "\n",
    "print(df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-10T03:28:46.722885900Z",
     "start_time": "2023-06-10T03:28:46.063973300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# 데이터셋 준비\n",
    "X = []  # 입력 데이터\n",
    "y = []  # 출력 데이터\n",
    "\n",
    "# 4개의 레코드씩 묶어서 처리\n",
    "for i in range(0, len(df) - 3, 4):\n",
    "    # gx, gy, gz 값 가져오기\n",
    "    gx_values = df.loc[i:i+3, 'scaled_gx'].values\n",
    "    gy_values = df.loc[i:i+3, 'scaled_gy'].values\n",
    "    gz_values = df.loc[i:i+3, 'scaled_gz'].values\n",
    "\n",
    "    # 레이블 값 가져오기\n",
    "    labels = df.loc[i:i+3, 'label'].values\n",
    "\n",
    "    # 서로 다른 레이블이 포함된 경우 해당 시퀀스는 분석에서 제외\n",
    "    if len(set(labels)) == 1:\n",
    "        # 시퀀스로 변환하여 입력 데이터에 추가\n",
    "        X.append([gx_values, gy_values, gz_values])\n",
    "\n",
    "        # 레이블 값 중복 제거하여 출력 데이터에 추가\n",
    "        y.append(labels[0])\n",
    "\n",
    "# 입력 데이터와 출력 데이터를 넘파이 배열로 변환\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# 출력 데이터를 One-Hot 인코딩\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(y)\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "y = onehot_encoder.fit_transform(integer_encoded)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-10T03:29:10.494307900Z",
     "start_time": "2023-06-10T03:28:55.090061500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2342/2342 [==============================] - 6s 2ms/step - loss: 1.5844 - accuracy: 0.2607\n",
      "Epoch 2/100\n",
      "2342/2342 [==============================] - 4s 2ms/step - loss: 1.3840 - accuracy: 0.3831\n",
      "Epoch 3/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 1.2626 - accuracy: 0.4300\n",
      "Epoch 4/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 1.1710 - accuracy: 0.4752\n",
      "Epoch 5/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 1.1174 - accuracy: 0.5034\n",
      "Epoch 6/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 1.0866 - accuracy: 0.5158\n",
      "Epoch 7/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 1.0637 - accuracy: 0.5273\n",
      "Epoch 8/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 1.0395 - accuracy: 0.5394\n",
      "Epoch 9/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 1.0027 - accuracy: 0.5595\n",
      "Epoch 10/100\n",
      "2342/2342 [==============================] - 4s 2ms/step - loss: 0.9848 - accuracy: 0.5702\n",
      "Epoch 11/100\n",
      "2342/2342 [==============================] - 4s 2ms/step - loss: 0.9724 - accuracy: 0.5756\n",
      "Epoch 12/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.9650 - accuracy: 0.5763\n",
      "Epoch 13/100\n",
      "2342/2342 [==============================] - 4s 2ms/step - loss: 0.9562 - accuracy: 0.5810\n",
      "Epoch 14/100\n",
      "2342/2342 [==============================] - 4s 2ms/step - loss: 0.9511 - accuracy: 0.5834\n",
      "Epoch 15/100\n",
      "2342/2342 [==============================] - 4s 2ms/step - loss: 0.9472 - accuracy: 0.5871\n",
      "Epoch 16/100\n",
      "2342/2342 [==============================] - 4s 2ms/step - loss: 0.9402 - accuracy: 0.5903\n",
      "Epoch 17/100\n",
      "2342/2342 [==============================] - 4s 2ms/step - loss: 0.9344 - accuracy: 0.5920\n",
      "Epoch 18/100\n",
      "2342/2342 [==============================] - 4s 2ms/step - loss: 0.9284 - accuracy: 0.5926\n",
      "Epoch 19/100\n",
      "2342/2342 [==============================] - 4s 2ms/step - loss: 0.9237 - accuracy: 0.5953\n",
      "Epoch 20/100\n",
      "2342/2342 [==============================] - 4s 2ms/step - loss: 0.9164 - accuracy: 0.5982\n",
      "Epoch 21/100\n",
      "2342/2342 [==============================] - 4s 2ms/step - loss: 0.9096 - accuracy: 0.6027\n",
      "Epoch 22/100\n",
      "2342/2342 [==============================] - 4s 2ms/step - loss: 0.9054 - accuracy: 0.6050\n",
      "Epoch 23/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.9011 - accuracy: 0.6066\n",
      "Epoch 24/100\n",
      "2342/2342 [==============================] - 4s 2ms/step - loss: 0.8960 - accuracy: 0.6082\n",
      "Epoch 25/100\n",
      "2342/2342 [==============================] - 4s 2ms/step - loss: 0.8910 - accuracy: 0.6099\n",
      "Epoch 26/100\n",
      "2342/2342 [==============================] - 4s 2ms/step - loss: 0.8867 - accuracy: 0.6125\n",
      "Epoch 27/100\n",
      "2342/2342 [==============================] - 4s 2ms/step - loss: 0.8823 - accuracy: 0.6124\n",
      "Epoch 28/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.8774 - accuracy: 0.6179\n",
      "Epoch 29/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.8748 - accuracy: 0.6179\n",
      "Epoch 30/100\n",
      "2342/2342 [==============================] - 4s 2ms/step - loss: 0.8724 - accuracy: 0.6184\n",
      "Epoch 31/100\n",
      "2342/2342 [==============================] - 4s 2ms/step - loss: 0.8669 - accuracy: 0.6201\n",
      "Epoch 32/100\n",
      "2342/2342 [==============================] - 4s 2ms/step - loss: 0.8637 - accuracy: 0.6228\n",
      "Epoch 33/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.8616 - accuracy: 0.6231\n",
      "Epoch 34/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.8587 - accuracy: 0.6254\n",
      "Epoch 35/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.8549 - accuracy: 0.6269\n",
      "Epoch 36/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.8529 - accuracy: 0.6274\n",
      "Epoch 37/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.8483 - accuracy: 0.6293\n",
      "Epoch 38/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.8454 - accuracy: 0.6317\n",
      "Epoch 39/100\n",
      "2342/2342 [==============================] - 4s 2ms/step - loss: 0.8430 - accuracy: 0.6301\n",
      "Epoch 40/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.8417 - accuracy: 0.6312\n",
      "Epoch 41/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.8404 - accuracy: 0.6306\n",
      "Epoch 42/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.8380 - accuracy: 0.6337\n",
      "Epoch 43/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.8340 - accuracy: 0.6372\n",
      "Epoch 44/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.8340 - accuracy: 0.6348\n",
      "Epoch 45/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.8306 - accuracy: 0.6363\n",
      "Epoch 46/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.8276 - accuracy: 0.6368\n",
      "Epoch 47/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.8259 - accuracy: 0.6398\n",
      "Epoch 48/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.8234 - accuracy: 0.6400\n",
      "Epoch 49/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.8216 - accuracy: 0.6400\n",
      "Epoch 50/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.8198 - accuracy: 0.6417\n",
      "Epoch 51/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.8176 - accuracy: 0.6427\n",
      "Epoch 52/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.8165 - accuracy: 0.6410\n",
      "Epoch 53/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.8143 - accuracy: 0.6441\n",
      "Epoch 54/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.8127 - accuracy: 0.6440\n",
      "Epoch 55/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.8101 - accuracy: 0.6451\n",
      "Epoch 56/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.8070 - accuracy: 0.6458\n",
      "Epoch 57/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.8062 - accuracy: 0.6467\n",
      "Epoch 58/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.8030 - accuracy: 0.6486\n",
      "Epoch 59/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.8034 - accuracy: 0.6491\n",
      "Epoch 60/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.8005 - accuracy: 0.6491\n",
      "Epoch 61/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.7992 - accuracy: 0.6511\n",
      "Epoch 62/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.7967 - accuracy: 0.6506\n",
      "Epoch 63/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.7958 - accuracy: 0.6525\n",
      "Epoch 64/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.7948 - accuracy: 0.6503\n",
      "Epoch 65/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.7917 - accuracy: 0.6529\n",
      "Epoch 66/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.7901 - accuracy: 0.6547\n",
      "Epoch 67/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.7885 - accuracy: 0.6542\n",
      "Epoch 68/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.7870 - accuracy: 0.6551\n",
      "Epoch 69/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.7859 - accuracy: 0.6565\n",
      "Epoch 70/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.7845 - accuracy: 0.6564\n",
      "Epoch 71/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.7804 - accuracy: 0.6595\n",
      "Epoch 72/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.7811 - accuracy: 0.6582\n",
      "Epoch 73/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.7790 - accuracy: 0.6584\n",
      "Epoch 74/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.7771 - accuracy: 0.6622\n",
      "Epoch 75/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.7767 - accuracy: 0.6602\n",
      "Epoch 76/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.7755 - accuracy: 0.6604\n",
      "Epoch 77/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.7728 - accuracy: 0.6610\n",
      "Epoch 78/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.7736 - accuracy: 0.6610\n",
      "Epoch 79/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.7706 - accuracy: 0.6623\n",
      "Epoch 80/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.7702 - accuracy: 0.6637\n",
      "Epoch 81/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.7684 - accuracy: 0.6625\n",
      "Epoch 82/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.7683 - accuracy: 0.6626\n",
      "Epoch 83/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.7647 - accuracy: 0.6664\n",
      "Epoch 84/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.7647 - accuracy: 0.6651\n",
      "Epoch 85/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.7624 - accuracy: 0.6680\n",
      "Epoch 86/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.7612 - accuracy: 0.6670\n",
      "Epoch 87/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.7611 - accuracy: 0.6667\n",
      "Epoch 88/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.7589 - accuracy: 0.6686\n",
      "Epoch 89/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.7573 - accuracy: 0.6694\n",
      "Epoch 90/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.7555 - accuracy: 0.6705\n",
      "Epoch 91/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.7547 - accuracy: 0.6700\n",
      "Epoch 92/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.7544 - accuracy: 0.6687\n",
      "Epoch 93/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.7514 - accuracy: 0.6740\n",
      "Epoch 94/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.7521 - accuracy: 0.6713\n",
      "Epoch 95/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.7482 - accuracy: 0.6751\n",
      "Epoch 96/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.7480 - accuracy: 0.6726\n",
      "Epoch 97/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.7476 - accuracy: 0.6738\n",
      "Epoch 98/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.7454 - accuracy: 0.6758\n",
      "Epoch 99/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.7445 - accuracy: 0.6748\n",
      "Epoch 100/100\n",
      "2342/2342 [==============================] - 5s 2ms/step - loss: 0.7440 - accuracy: 0.6743\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "# LSTM 모델 정의\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.LSTM(128, input_shape=(3, 4)))  # 3개의 시퀀스, 각 시퀀스에 4개의 피처\n",
    "model.add(keras.layers.Dense(5, activation='softmax'))  # 분류할 클래스 수에 맞게 조정\n",
    "\n",
    "# 모델 컴파일 및 학습\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X, y, epochs=100, batch_size=32)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-10T03:36:59.228600300Z",
     "start_time": "2023-06-10T03:29:14.113352500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# 최상의 적합 모델의 가중치를 피클 파일로 저장\n",
    "best_model_weights = history.model.get_weights()\n",
    "pickle.dump(best_model_weights, open('../model/best_model_weights.pkl', 'wb'))\n",
    "\n",
    "# 레이블 인코더의 클래스 정보를 저장\n",
    "np.save('../model/label_encoder_classes.npy', label_encoder.classes_)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-09T02:42:11.394592500Z",
     "start_time": "2023-06-09T02:42:11.381249200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import pickle\n",
    "\n",
    "# Define the Loaded Model\n",
    "loaded_model = keras.models.Sequential()\n",
    "loaded_model.add(keras.layers.LSTM(128, input_shape=(3, 4)))\n",
    "loaded_model.add(keras.layers.Dense(5, activation='softmax'))\n",
    "\n",
    "# Load the Model Weights\n",
    "loaded_model_weights = pickle.load(open('../model/best_model_weights.pkl', 'rb'))\n",
    "loaded_model.set_weights(loaded_model_weights)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-09T02:42:13.255681600Z",
     "start_time": "2023-06-09T02:42:13.012249300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# Compile the Loaded Model\n",
    "loaded_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Save the Loaded Model\n",
    "loaded_model.save('../model/loaded_model.h5')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-09T02:42:15.139648700Z",
     "start_time": "2023-06-09T02:42:15.073332700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000021AEAA1A5F0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000021AEAA1B250> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "['fall' 'jogging' 'jogging' 'stairs_walking' 'stairs_walking' 'fall'\n",
      " 'stairs_walking' 'fall' 'stairs_walking' 'stairs_walking'\n",
      " 'stairs_walking' 'fall' 'fall' 'fall' 'fall' 'fall' 'fall' 'fall' 'fall'\n",
      " 'stairs_walking' 'jogging' 'fall' 'stairs_walking' 'stairs_walking'\n",
      " 'fall' 'fall' 'fall' 'jogging' 'stairs_walking' 'fall' 'fall'\n",
      " 'stairs_walking' 'fall' 'fall' 'jogging' 'jogging' 'fall' 'fall'\n",
      " 'stairs_walking' 'jogging' 'stairs_walking' 'fall' 'jogging' 'fall'\n",
      " 'fall' 'stairs_walking' 'fall' 'fall' 'fall' 'fall']\n"
     ]
    }
   ],
   "source": [
    "# Load the data for prediction\n",
    "pred_df = pd.read_csv('../data/Arduino/fall_sc.csv')\n",
    "X_pred = []\n",
    "\n",
    "# Prepare the data for prediction\n",
    "for i in range(0, len(pred_df) - 3, 4):\n",
    "    gx_values = pred_df.loc[i:i+3, 'scaled_gx'].values\n",
    "    gy_values = pred_df.loc[i:i+3, 'scaled_gy'].values\n",
    "    gz_values = pred_df.loc[i:i+3, 'scaled_gz'].values\n",
    "    X_pred.append([gx_values, gy_values, gz_values])\n",
    "\n",
    "X_pred = np.array(X_pred)\n",
    "\n",
    "# Load the Trained Model and Make Predictions\n",
    "try:\n",
    "    best_model = keras.models.load_model('../model/loaded_model.h5')\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"모델 파일을 찾을 수 없습니다: {e}\")\n",
    "    exit()\n",
    "\n",
    "predictions = best_model.predict(X_pred)\n",
    "\n",
    "# Decode and Store the Predictions\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.classes_ = np.load('../model/label_encoder_classes.npy')\n",
    "decoded_predictions = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
    "\n",
    "pred_df['predicted_label'] = np.repeat(decoded_predictions, 4)\n",
    "pred_df.to_csv('../data/prediction_results.csv', index=False)\n",
    "\n",
    "# 예측\n",
    "predictions = loaded_model.predict(X_pred)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "predicted_classes = label_encoder.inverse_transform(predicted_labels)\n",
    "\n",
    "# 예측 결과 출력\n",
    "print(predicted_classes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-09T03:52:36.052963500Z",
     "start_time": "2023-06-09T03:52:34.452024900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# 데이터셋 준비\n",
    "X = []  # 입력 데이터\n",
    "y = []  # 출력 데이터\n",
    "\n",
    "# 40개의 레코드씩 묶어서 처리\n",
    "for i in range(0, len(df) - 39, 40):\n",
    "    # gx, gy, gz 값 가져오기\n",
    "    gx_values = df.loc[i:i+39, 'scaled_gx'].values\n",
    "    gy_values = df.loc[i:i+39, 'scaled_gy'].values\n",
    "    gz_values = df.loc[i:i+39, 'scaled_gz'].values\n",
    "\n",
    "    # 레이블 값 가져오기\n",
    "    labels = df.loc[i:i+39, 'label'].values\n",
    "\n",
    "    # 서로 다른 레이블이 포함된 경우 해당 시퀀스는 분석에서 제외\n",
    "    if len(set(labels)) == 1:\n",
    "        # 시퀀스로 변환하여 입력 데이터에 추가\n",
    "        X.append([gx_values, gy_values, gz_values])\n",
    "\n",
    "        # 레이블 값 중복 제거하여 출력 데이터에 추가\n",
    "        y.append(labels[0])\n",
    "\n",
    "# 입력 데이터와 출력 데이터를 넘파이 배열로 변환\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# 출력 데이터를 One-Hot 인코딩\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(y)\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "y = onehot_encoder.fit_transform(integer_encoded)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-09T08:35:51.983168400Z",
     "start_time": "2023-06-09T08:35:50.421948400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "256/256 [==============================] - 5s 14ms/step - loss: 1.5560 - accuracy: 0.2877\n",
      "Epoch 2/100\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 1.5522 - accuracy: 0.2877\n",
      "Epoch 3/100\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 1.5502 - accuracy: 0.2877\n",
      "Epoch 4/100\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 1.5498 - accuracy: 0.2877\n",
      "Epoch 5/100\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 1.5494 - accuracy: 0.2861\n",
      "Epoch 6/100\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 1.5463 - accuracy: 0.2915\n",
      "Epoch 7/100\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 1.5193 - accuracy: 0.3295\n",
      "Epoch 8/100\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 1.2886 - accuracy: 0.4414\n",
      "Epoch 9/100\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 1.1097 - accuracy: 0.5001\n",
      "Epoch 10/100\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 1.0480 - accuracy: 0.5279\n",
      "Epoch 11/100\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 0.9827 - accuracy: 0.5643\n",
      "Epoch 12/100\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 0.9446 - accuracy: 0.5773\n",
      "Epoch 13/100\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 0.8787 - accuracy: 0.5988\n",
      "Epoch 14/100\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 0.8864 - accuracy: 0.5969\n",
      "Epoch 15/100\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 0.8643 - accuracy: 0.6010\n",
      "Epoch 16/100\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 0.8373 - accuracy: 0.6145\n",
      "Epoch 17/100\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 0.8018 - accuracy: 0.6435\n",
      "Epoch 18/100\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 0.7802 - accuracy: 0.6570\n",
      "Epoch 19/100\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 0.7540 - accuracy: 0.6716\n",
      "Epoch 20/100\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 0.7311 - accuracy: 0.6866\n",
      "Epoch 21/100\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 0.7383 - accuracy: 0.6902\n",
      "Epoch 22/100\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 0.7031 - accuracy: 0.7096\n",
      "Epoch 23/100\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 0.7257 - accuracy: 0.7056\n",
      "Epoch 24/100\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 0.7267 - accuracy: 0.6967\n",
      "Epoch 25/100\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 0.6712 - accuracy: 0.7205\n",
      "Epoch 26/100\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 0.6626 - accuracy: 0.7287\n",
      "Epoch 27/100\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 0.6436 - accuracy: 0.7326\n",
      "Epoch 28/100\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 0.6369 - accuracy: 0.7397\n",
      "Epoch 29/100\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 0.6285 - accuracy: 0.7430\n",
      "Epoch 30/100\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 0.6162 - accuracy: 0.7444\n",
      "Epoch 31/100\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 0.6044 - accuracy: 0.7505\n",
      "Epoch 32/100\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 0.6003 - accuracy: 0.7542\n",
      "Epoch 33/100\n",
      "256/256 [==============================] - 4s 15ms/step - loss: 0.5893 - accuracy: 0.7623\n",
      "Epoch 34/100\n",
      "256/256 [==============================] - 4s 15ms/step - loss: 0.5664 - accuracy: 0.7677\n",
      "Epoch 35/100\n",
      "256/256 [==============================] - 4s 15ms/step - loss: 0.5566 - accuracy: 0.7736\n",
      "Epoch 36/100\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 0.5439 - accuracy: 0.7775\n",
      "Epoch 37/100\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 0.5151 - accuracy: 0.7906\n",
      "Epoch 38/100\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 0.5179 - accuracy: 0.7938\n",
      "Epoch 39/100\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 0.4829 - accuracy: 0.8022\n",
      "Epoch 40/100\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 0.4735 - accuracy: 0.8071\n",
      "Epoch 41/100\n",
      "256/256 [==============================] - 4s 15ms/step - loss: 0.4507 - accuracy: 0.8137\n",
      "Epoch 42/100\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 0.4422 - accuracy: 0.8140\n",
      "Epoch 43/100\n",
      "256/256 [==============================] - 4s 15ms/step - loss: 0.4241 - accuracy: 0.8213\n",
      "Epoch 44/100\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 0.4959 - accuracy: 0.7977\n",
      "Epoch 45/100\n",
      "256/256 [==============================] - 4s 15ms/step - loss: 0.4494 - accuracy: 0.8151\n",
      "Epoch 46/100\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 0.4306 - accuracy: 0.8260\n",
      "Epoch 47/100\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 0.4118 - accuracy: 0.8349\n",
      "Epoch 48/100\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 0.3992 - accuracy: 0.8368\n",
      "Epoch 49/100\n",
      "256/256 [==============================] - 4s 15ms/step - loss: 0.3975 - accuracy: 0.8348\n",
      "Epoch 50/100\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 0.3834 - accuracy: 0.8435\n",
      "Epoch 51/100\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 0.3787 - accuracy: 0.8452\n",
      "Epoch 52/100\n",
      "256/256 [==============================] - 4s 15ms/step - loss: 0.3720 - accuracy: 0.8471\n",
      "Epoch 53/100\n",
      "256/256 [==============================] - 4s 15ms/step - loss: 0.3564 - accuracy: 0.8539\n",
      "Epoch 54/100\n",
      "256/256 [==============================] - 4s 15ms/step - loss: 0.3643 - accuracy: 0.8473\n",
      "Epoch 55/100\n",
      "256/256 [==============================] - 4s 15ms/step - loss: 0.3547 - accuracy: 0.8554\n",
      "Epoch 56/100\n",
      "256/256 [==============================] - 4s 15ms/step - loss: 0.3374 - accuracy: 0.8618\n",
      "Epoch 57/100\n",
      "256/256 [==============================] - 4s 15ms/step - loss: 0.3440 - accuracy: 0.8585\n",
      "Epoch 58/100\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 0.3310 - accuracy: 0.8679\n",
      "Epoch 59/100\n",
      "256/256 [==============================] - 4s 15ms/step - loss: 0.3453 - accuracy: 0.8590\n",
      "Epoch 60/100\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 0.3253 - accuracy: 0.8680\n",
      "Epoch 61/100\n",
      "256/256 [==============================] - 4s 15ms/step - loss: 0.3149 - accuracy: 0.8712\n",
      "Epoch 62/100\n",
      "256/256 [==============================] - 4s 15ms/step - loss: 0.3176 - accuracy: 0.8721\n",
      "Epoch 63/100\n",
      "256/256 [==============================] - 4s 15ms/step - loss: 0.3064 - accuracy: 0.8739\n",
      "Epoch 64/100\n",
      "256/256 [==============================] - 4s 15ms/step - loss: 0.2970 - accuracy: 0.8764\n",
      "Epoch 65/100\n",
      "256/256 [==============================] - 4s 15ms/step - loss: 0.2927 - accuracy: 0.8782\n",
      "Epoch 66/100\n",
      "256/256 [==============================] - 4s 15ms/step - loss: 0.2865 - accuracy: 0.8782\n",
      "Epoch 67/100\n",
      "256/256 [==============================] - 4s 15ms/step - loss: 0.2982 - accuracy: 0.8749\n",
      "Epoch 68/100\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 0.2818 - accuracy: 0.8842\n",
      "Epoch 69/100\n",
      "256/256 [==============================] - 4s 15ms/step - loss: 0.2722 - accuracy: 0.8873\n",
      "Epoch 70/100\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 0.2700 - accuracy: 0.8897\n",
      "Epoch 71/100\n",
      "256/256 [==============================] - 4s 15ms/step - loss: 0.2672 - accuracy: 0.8900\n",
      "Epoch 72/100\n",
      "256/256 [==============================] - 4s 15ms/step - loss: 0.2495 - accuracy: 0.8935\n",
      "Epoch 73/100\n",
      "256/256 [==============================] - 4s 15ms/step - loss: 0.2548 - accuracy: 0.8925\n",
      "Epoch 74/100\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 0.2533 - accuracy: 0.8906\n",
      "Epoch 75/100\n",
      "256/256 [==============================] - 4s 15ms/step - loss: 0.2569 - accuracy: 0.8922\n",
      "Epoch 76/100\n",
      "256/256 [==============================] - 4s 15ms/step - loss: 0.2411 - accuracy: 0.8973\n",
      "Epoch 77/100\n",
      "256/256 [==============================] - 4s 15ms/step - loss: 0.2270 - accuracy: 0.9044\n",
      "Epoch 78/100\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 0.2318 - accuracy: 0.9015\n",
      "Epoch 79/100\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 0.2201 - accuracy: 0.9055\n",
      "Epoch 80/100\n",
      "256/256 [==============================] - 4s 15ms/step - loss: 0.2162 - accuracy: 0.9070\n",
      "Epoch 81/100\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 0.2303 - accuracy: 0.9021\n",
      "Epoch 82/100\n",
      "256/256 [==============================] - 4s 15ms/step - loss: 0.2113 - accuracy: 0.9107\n",
      "Epoch 83/100\n",
      "256/256 [==============================] - 4s 15ms/step - loss: 0.2105 - accuracy: 0.9079\n",
      "Epoch 84/100\n",
      "256/256 [==============================] - 4s 15ms/step - loss: 0.2268 - accuracy: 0.9019\n",
      "Epoch 85/100\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 0.2026 - accuracy: 0.9169\n",
      "Epoch 86/100\n",
      "256/256 [==============================] - 4s 15ms/step - loss: 0.2015 - accuracy: 0.9139\n",
      "Epoch 87/100\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 0.2037 - accuracy: 0.9131\n",
      "Epoch 88/100\n",
      "256/256 [==============================] - 4s 15ms/step - loss: 0.2011 - accuracy: 0.9142\n",
      "Epoch 89/100\n",
      "256/256 [==============================] - 4s 15ms/step - loss: 0.2019 - accuracy: 0.9121\n",
      "Epoch 90/100\n",
      "256/256 [==============================] - 4s 15ms/step - loss: 0.2014 - accuracy: 0.9115\n",
      "Epoch 91/100\n",
      "256/256 [==============================] - 4s 15ms/step - loss: 0.1945 - accuracy: 0.9150\n",
      "Epoch 92/100\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 0.1843 - accuracy: 0.9183\n",
      "Epoch 93/100\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 0.1748 - accuracy: 0.9206\n",
      "Epoch 94/100\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 0.2264 - accuracy: 0.9032\n",
      "Epoch 95/100\n",
      "256/256 [==============================] - 4s 15ms/step - loss: 0.1892 - accuracy: 0.9181\n",
      "Epoch 96/100\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 0.1865 - accuracy: 0.9152\n",
      "Epoch 97/100\n",
      "256/256 [==============================] - 4s 15ms/step - loss: 0.1807 - accuracy: 0.9219\n",
      "Epoch 98/100\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 0.1640 - accuracy: 0.9293\n",
      "Epoch 99/100\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 0.1570 - accuracy: 0.9286\n",
      "Epoch 100/100\n",
      "256/256 [==============================] - 4s 14ms/step - loss: 0.1760 - accuracy: 0.9216\n"
     ]
    }
   ],
   "source": [
    "X = np.transpose(X, (0, 2, 1))  # 입력 데이터의 축 순서 변경\n",
    "\n",
    "# LSTM 모델 정의\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.LSTM(128, input_shape=(40, 3)))  # 40개의 시퀀스, 각 시퀀스에 3개의 피처\n",
    "model.add(keras.layers.Dense(5, activation='softmax'))  # 분류할 클래스 수에 맞게 조정\n",
    "\n",
    "# 모델 컴파일 및 학습\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X, y, epochs=100, batch_size=32)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-09T08:42:10.263507800Z",
     "start_time": "2023-06-09T08:35:59.080031900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# 최상의 적합 모델의 가중치를 피클 파일로 저장\n",
    "best_model_weights = history.model.get_weights()\n",
    "pickle.dump(best_model_weights, open('../model/best_model_weights.pkl', 'wb'))\n",
    "\n",
    "# 레이블 인코더의 클래스 정보를 저장\n",
    "np.save('../model/label_encoder_classes.npy', label_encoder.classes_)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-09T08:42:34.500407300Z",
     "start_time": "2023-06-09T08:42:34.485635Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import pickle\n",
    "\n",
    "# Define the Loaded Model\n",
    "loaded_model = keras.models.Sequential()\n",
    "loaded_model.add(keras.layers.LSTM(128, input_shape=(40, 3)))  # 입력 형태 수정\n",
    "loaded_model.add(keras.layers.Dense(5, activation='softmax'))\n",
    "\n",
    "# Load the Model Weights\n",
    "loaded_model_weights = pickle.load(open('../model/best_model_weights.pkl', 'rb'))\n",
    "loaded_model.set_weights(loaded_model_weights)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-09T08:42:36.549697Z",
     "start_time": "2023-06-09T08:42:36.291192900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "#Compile the Loaded Model\n",
    "loaded_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Save the Loaded Model\n",
    "loaded_model.save('../model/loaded_model.h5')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-09T08:42:49.269265200Z",
     "start_time": "2023-06-09T08:42:49.197132400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 603ms/step\n",
      "['fall' 'fall' 'fall' 'stairs_walking' 'fall']\n"
     ]
    }
   ],
   "source": [
    "# Load the data for prediction\n",
    "pred_df = pd.read_csv('../data/Arduino/fall_sc.csv')\n",
    "\n",
    "# Prepare the data for prediction\n",
    "X_pred = []\n",
    "for i in range(0, len(pred_df) - 39, 40):\n",
    "    gx_values = pred_df.loc[i:i+39, 'scaled_gx'].values.reshape((40, 1))\n",
    "    gy_values = pred_df.loc[i:i+39, 'scaled_gy'].values.reshape((40, 1))\n",
    "    gz_values = pred_df.loc[i:i+39, 'scaled_gz'].values.reshape((40, 1))\n",
    "    X_pred.append(np.concatenate((gx_values, gy_values, gz_values), axis=1))\n",
    "\n",
    "X_pred = np.array(X_pred)\n",
    "\n",
    "# Load the Trained Model and Make Predictions\n",
    "try:\n",
    "    loaded_model = keras.models.load_model('../model/loaded_model.h5')\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"모델 파일을 찾을 수 없습니다: {e}\")\n",
    "    exit()\n",
    "\n",
    "predictions = loaded_model.predict(X_pred)\n",
    "\n",
    "# Decode and Store the Predictions\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.classes_ = np.load('../model/label_encoder_classes.npy')\n",
    "decoded_predictions = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
    "\n",
    "# 예측 결과 출력\n",
    "print(decoded_predictions)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-09T08:42:52.313674800Z",
     "start_time": "2023-06-09T08:42:51.413058600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 538ms/step\n",
      "['jogging' 'jogging' 'fall' 'fall' 'jogging' 'fall' 'fall' 'fall' 'fall'\n",
      " 'stairs_walking']\n"
     ]
    }
   ],
   "source": [
    "# Load the data for prediction\n",
    "pred_df = pd.read_csv('../data/Arduino/walking_sc.csv')\n",
    "\n",
    "# Prepare the data for prediction\n",
    "X_pred = []\n",
    "for i in range(0, len(pred_df) - 39, 40):\n",
    "    gx_values = pred_df.loc[i:i+39, 'scaled_gx'].values.reshape((40, 1))\n",
    "    gy_values = pred_df.loc[i:i+39, 'scaled_gy'].values.reshape((40, 1))\n",
    "    gz_values = pred_df.loc[i:i+39, 'scaled_gz'].values.reshape((40, 1))\n",
    "    X_pred.append(np.concatenate((gx_values, gy_values, gz_values), axis=1))\n",
    "\n",
    "X_pred = np.array(X_pred)\n",
    "\n",
    "# Load the Trained Model and Make Predictions\n",
    "try:\n",
    "    loaded_model = keras.models.load_model('../model/loaded_model.h5')\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"모델 파일을 찾을 수 없습니다: {e}\")\n",
    "    exit()\n",
    "\n",
    "predictions = loaded_model.predict(X_pred)\n",
    "\n",
    "# Decode and Store the Predictions\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.classes_ = np.load('../model/label_encoder_classes.npy')\n",
    "decoded_predictions = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
    "\n",
    "# 예측 결과 출력\n",
    "print(decoded_predictions)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-09T08:42:58.609045900Z",
     "start_time": "2023-06-09T08:42:57.774122Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 531ms/step\n",
      "['fall' 'fall' 'stairs_walking' 'jogging' 'fall' 'fall']\n"
     ]
    }
   ],
   "source": [
    "# Load the data for prediction\n",
    "pred_df = pd.read_csv('../data/Arduino/standing_sc.csv')\n",
    "\n",
    "# Prepare the data for prediction\n",
    "X_pred = []\n",
    "for i in range(0, len(pred_df) - 39, 40):\n",
    "    gx_values = pred_df.loc[i:i+39, 'scaled_gx'].values.reshape((40, 1))\n",
    "    gy_values = pred_df.loc[i:i+39, 'scaled_gy'].values.reshape((40, 1))\n",
    "    gz_values = pred_df.loc[i:i+39, 'scaled_gz'].values.reshape((40, 1))\n",
    "    X_pred.append(np.concatenate((gx_values, gy_values, gz_values), axis=1))\n",
    "\n",
    "X_pred = np.array(X_pred)\n",
    "\n",
    "# Load the Trained Model and Make Predictions\n",
    "try:\n",
    "    loaded_model = keras.models.load_model('../model/loaded_model.h5')\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"모델 파일을 찾을 수 없습니다: {e}\")\n",
    "    exit()\n",
    "\n",
    "predictions = loaded_model.predict(X_pred)\n",
    "\n",
    "# Decode and Store the Predictions\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.classes_ = np.load('../model/label_encoder_classes.npy')\n",
    "decoded_predictions = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
    "\n",
    "# 예측 결과 출력\n",
    "print(decoded_predictions)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-09T08:43:00.875342300Z",
     "start_time": "2023-06-09T08:43:00.044550100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
