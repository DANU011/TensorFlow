{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "208/208 [==============================] - 4s 13ms/step - loss: 0.5986 - accuracy: 0.7678 - val_loss: 0.6321 - val_accuracy: 0.7338\n",
      "Epoch 2/100\n",
      "208/208 [==============================] - 2s 11ms/step - loss: 0.3975 - accuracy: 0.8459 - val_loss: 0.5843 - val_accuracy: 0.7708\n",
      "Epoch 3/100\n",
      "208/208 [==============================] - 2s 11ms/step - loss: 0.3345 - accuracy: 0.8697 - val_loss: 0.5830 - val_accuracy: 0.7869\n",
      "Epoch 4/100\n",
      "208/208 [==============================] - 2s 12ms/step - loss: 0.2858 - accuracy: 0.8881 - val_loss: 0.5672 - val_accuracy: 0.8050\n",
      "Epoch 5/100\n",
      "208/208 [==============================] - 2s 11ms/step - loss: 0.2557 - accuracy: 0.8989 - val_loss: 0.5223 - val_accuracy: 0.8246\n",
      "Epoch 6/100\n",
      "208/208 [==============================] - 2s 11ms/step - loss: 0.2172 - accuracy: 0.9130 - val_loss: 0.4944 - val_accuracy: 0.8239\n",
      "Epoch 7/100\n",
      "208/208 [==============================] - 2s 11ms/step - loss: 0.1916 - accuracy: 0.9240 - val_loss: 0.5389 - val_accuracy: 0.8316\n",
      "Epoch 8/100\n",
      "208/208 [==============================] - 2s 12ms/step - loss: 0.1757 - accuracy: 0.9272 - val_loss: 0.5390 - val_accuracy: 0.8288\n",
      "Epoch 9/100\n",
      "208/208 [==============================] - 2s 11ms/step - loss: 0.1575 - accuracy: 0.9401 - val_loss: 0.5707 - val_accuracy: 0.8323\n",
      "Epoch 10/100\n",
      "208/208 [==============================] - 2s 11ms/step - loss: 0.1546 - accuracy: 0.9365 - val_loss: 0.5680 - val_accuracy: 0.8302\n",
      "Epoch 11/100\n",
      "208/208 [==============================] - 2s 11ms/step - loss: 0.1402 - accuracy: 0.9422 - val_loss: 0.5817 - val_accuracy: 0.8246\n",
      "Epoch 12/100\n",
      "208/208 [==============================] - 2s 11ms/step - loss: 0.1176 - accuracy: 0.9519 - val_loss: 0.5828 - val_accuracy: 0.8232\n",
      "Epoch 13/100\n",
      "208/208 [==============================] - 2s 11ms/step - loss: 0.1044 - accuracy: 0.9553 - val_loss: 0.6260 - val_accuracy: 0.8218\n",
      "Epoch 14/100\n",
      "208/208 [==============================] - 2s 11ms/step - loss: 0.1190 - accuracy: 0.9505 - val_loss: 0.6171 - val_accuracy: 0.8246\n",
      "Epoch 15/100\n",
      "208/208 [==============================] - 2s 11ms/step - loss: 0.1054 - accuracy: 0.9562 - val_loss: 0.6306 - val_accuracy: 0.8148\n",
      "Epoch 16/100\n",
      "208/208 [==============================] - 2s 11ms/step - loss: 0.1034 - accuracy: 0.9556 - val_loss: 0.5990 - val_accuracy: 0.8330\n",
      "Epoch 17/100\n",
      "208/208 [==============================] - 2s 11ms/step - loss: 0.1112 - accuracy: 0.9562 - val_loss: 0.6433 - val_accuracy: 0.8099\n",
      "Epoch 18/100\n",
      "208/208 [==============================] - 2s 11ms/step - loss: 0.0888 - accuracy: 0.9607 - val_loss: 0.6528 - val_accuracy: 0.8253\n",
      "Epoch 19/100\n",
      "208/208 [==============================] - 2s 11ms/step - loss: 0.0788 - accuracy: 0.9676 - val_loss: 0.7636 - val_accuracy: 0.8043\n",
      "Epoch 20/100\n",
      "208/208 [==============================] - 2s 11ms/step - loss: 0.0805 - accuracy: 0.9640 - val_loss: 0.7269 - val_accuracy: 0.8162\n",
      "Epoch 21/100\n",
      "208/208 [==============================] - 2s 12ms/step - loss: 0.0799 - accuracy: 0.9666 - val_loss: 0.6447 - val_accuracy: 0.8337\n",
      "Epoch 22/100\n",
      "208/208 [==============================] - 2s 12ms/step - loss: 0.0747 - accuracy: 0.9678 - val_loss: 0.6812 - val_accuracy: 0.8365\n",
      "Epoch 23/100\n",
      "208/208 [==============================] - 2s 11ms/step - loss: 0.0710 - accuracy: 0.9719 - val_loss: 0.7076 - val_accuracy: 0.8302\n",
      "Epoch 24/100\n",
      "208/208 [==============================] - 2s 11ms/step - loss: 0.0878 - accuracy: 0.9645 - val_loss: 0.7094 - val_accuracy: 0.8071\n",
      "Epoch 25/100\n",
      "208/208 [==============================] - 2s 11ms/step - loss: 0.0968 - accuracy: 0.9622 - val_loss: 0.8596 - val_accuracy: 0.7841\n",
      "Epoch 26/100\n",
      "208/208 [==============================] - 2s 11ms/step - loss: 0.0878 - accuracy: 0.9657 - val_loss: 0.8279 - val_accuracy: 0.8022\n",
      "Epoch 27/100\n",
      "208/208 [==============================] - 2s 11ms/step - loss: 0.0819 - accuracy: 0.9661 - val_loss: 0.7389 - val_accuracy: 0.8204\n",
      "Epoch 28/100\n",
      "208/208 [==============================] - 2s 11ms/step - loss: 0.0635 - accuracy: 0.9773 - val_loss: 0.7409 - val_accuracy: 0.8190\n",
      "Epoch 29/100\n",
      "208/208 [==============================] - 2s 12ms/step - loss: 0.0612 - accuracy: 0.9749 - val_loss: 0.7342 - val_accuracy: 0.8302\n",
      "Epoch 30/100\n",
      "208/208 [==============================] - 2s 11ms/step - loss: 0.0535 - accuracy: 0.9791 - val_loss: 0.7558 - val_accuracy: 0.8211\n",
      "Epoch 31/100\n",
      "208/208 [==============================] - 2s 11ms/step - loss: 0.0522 - accuracy: 0.9767 - val_loss: 0.7462 - val_accuracy: 0.8400\n",
      "Epoch 32/100\n",
      "208/208 [==============================] - 2s 12ms/step - loss: 0.0456 - accuracy: 0.9803 - val_loss: 0.7715 - val_accuracy: 0.8372\n",
      "Epoch 33/100\n",
      "208/208 [==============================] - 2s 11ms/step - loss: 0.0496 - accuracy: 0.9791 - val_loss: 0.8267 - val_accuracy: 0.8190\n",
      "Epoch 34/100\n",
      "208/208 [==============================] - 2s 12ms/step - loss: 0.0841 - accuracy: 0.9678 - val_loss: 0.7454 - val_accuracy: 0.8267\n",
      "Epoch 35/100\n",
      "208/208 [==============================] - 2s 12ms/step - loss: 0.0968 - accuracy: 0.9637 - val_loss: 0.9164 - val_accuracy: 0.7932\n",
      "Epoch 36/100\n",
      "208/208 [==============================] - 2s 11ms/step - loss: 0.0971 - accuracy: 0.9631 - val_loss: 0.7755 - val_accuracy: 0.8176\n",
      "Epoch 37/100\n",
      "208/208 [==============================] - 2s 11ms/step - loss: 0.0648 - accuracy: 0.9744 - val_loss: 0.7724 - val_accuracy: 0.8309\n",
      "Epoch 38/100\n",
      "208/208 [==============================] - 2s 11ms/step - loss: 0.0532 - accuracy: 0.9791 - val_loss: 0.8768 - val_accuracy: 0.8246\n",
      "Epoch 39/100\n",
      "208/208 [==============================] - 2s 11ms/step - loss: 0.0473 - accuracy: 0.9822 - val_loss: 0.8071 - val_accuracy: 0.8330\n",
      "Epoch 40/100\n",
      "208/208 [==============================] - 2s 11ms/step - loss: 0.0420 - accuracy: 0.9841 - val_loss: 0.8398 - val_accuracy: 0.8323\n",
      "Epoch 41/100\n",
      "208/208 [==============================] - 2s 11ms/step - loss: 0.0342 - accuracy: 0.9874 - val_loss: 0.8837 - val_accuracy: 0.8260\n",
      "Epoch 42/100\n",
      "208/208 [==============================] - 2s 11ms/step - loss: 0.0376 - accuracy: 0.9847 - val_loss: 0.8465 - val_accuracy: 0.8351\n",
      "Epoch 43/100\n",
      "208/208 [==============================] - 2s 12ms/step - loss: 0.0351 - accuracy: 0.9868 - val_loss: 0.8815 - val_accuracy: 0.8337\n",
      "Epoch 44/100\n",
      "208/208 [==============================] - 2s 11ms/step - loss: 0.0324 - accuracy: 0.9874 - val_loss: 0.8378 - val_accuracy: 0.8400\n",
      "Epoch 45/100\n",
      "208/208 [==============================] - 2s 11ms/step - loss: 0.0338 - accuracy: 0.9880 - val_loss: 0.8551 - val_accuracy: 0.8183\n",
      "Epoch 46/100\n",
      "208/208 [==============================] - 2s 11ms/step - loss: 0.0319 - accuracy: 0.9892 - val_loss: 0.9161 - val_accuracy: 0.8253\n",
      "Epoch 47/100\n",
      "208/208 [==============================] - 2s 12ms/step - loss: 0.0718 - accuracy: 0.9738 - val_loss: 0.7353 - val_accuracy: 0.8148\n",
      "Epoch 48/100\n",
      "208/208 [==============================] - 2s 12ms/step - loss: 0.0820 - accuracy: 0.9692 - val_loss: 0.8108 - val_accuracy: 0.8162\n",
      "Epoch 49/100\n",
      "208/208 [==============================] - 2s 11ms/step - loss: 0.0506 - accuracy: 0.9828 - val_loss: 0.8042 - val_accuracy: 0.8267\n",
      "Epoch 50/100\n",
      "208/208 [==============================] - 2s 12ms/step - loss: 0.0298 - accuracy: 0.9898 - val_loss: 0.8581 - val_accuracy: 0.8344\n",
      "Epoch 51/100\n",
      "208/208 [==============================] - 2s 11ms/step - loss: 0.0276 - accuracy: 0.9895 - val_loss: 0.9125 - val_accuracy: 0.8351\n",
      "Epoch 52/100\n",
      "208/208 [==============================] - 2s 12ms/step - loss: 0.0246 - accuracy: 0.9911 - val_loss: 0.8882 - val_accuracy: 0.8372\n",
      "Epoch 53/100\n",
      "208/208 [==============================] - 2s 11ms/step - loss: 0.0226 - accuracy: 0.9902 - val_loss: 0.9127 - val_accuracy: 0.8330\n",
      "Epoch 54/100\n",
      "208/208 [==============================] - 2s 12ms/step - loss: 0.0411 - accuracy: 0.9856 - val_loss: 0.9304 - val_accuracy: 0.8246\n",
      "Epoch 55/100\n",
      "208/208 [==============================] - 2s 11ms/step - loss: 0.0862 - accuracy: 0.9690 - val_loss: 0.8734 - val_accuracy: 0.8239\n",
      "Epoch 56/100\n",
      "208/208 [==============================] - 2s 11ms/step - loss: 0.0361 - accuracy: 0.9875 - val_loss: 0.9071 - val_accuracy: 0.8197\n",
      "Epoch 57/100\n",
      "208/208 [==============================] - 2s 11ms/step - loss: 0.0295 - accuracy: 0.9892 - val_loss: 0.8794 - val_accuracy: 0.8232\n",
      "Epoch 58/100\n",
      "208/208 [==============================] - 2s 12ms/step - loss: 0.0197 - accuracy: 0.9926 - val_loss: 0.8821 - val_accuracy: 0.8365\n",
      "Epoch 59/100\n",
      "208/208 [==============================] - 2s 11ms/step - loss: 0.0175 - accuracy: 0.9932 - val_loss: 0.9017 - val_accuracy: 0.8274\n",
      "Epoch 60/100\n",
      "208/208 [==============================] - 2s 11ms/step - loss: 0.0168 - accuracy: 0.9932 - val_loss: 0.8882 - val_accuracy: 0.8393\n",
      "Epoch 61/100\n",
      "208/208 [==============================] - 2s 11ms/step - loss: 0.0139 - accuracy: 0.9952 - val_loss: 0.9000 - val_accuracy: 0.8365\n",
      "Epoch 62/100\n",
      "208/208 [==============================] - 2s 12ms/step - loss: 0.0196 - accuracy: 0.9931 - val_loss: 0.9326 - val_accuracy: 0.8274\n",
      "Epoch 63/100\n",
      "208/208 [==============================] - 2s 12ms/step - loss: 0.0165 - accuracy: 0.9937 - val_loss: 0.9069 - val_accuracy: 0.8316\n",
      "Epoch 64/100\n",
      "208/208 [==============================] - 2s 12ms/step - loss: 0.0431 - accuracy: 0.9841 - val_loss: 0.9971 - val_accuracy: 0.8162\n",
      "Epoch 65/100\n",
      "208/208 [==============================] - 2s 11ms/step - loss: 0.0941 - accuracy: 0.9657 - val_loss: 0.8492 - val_accuracy: 0.7987\n",
      "Epoch 66/100\n",
      "208/208 [==============================] - 2s 12ms/step - loss: 0.0612 - accuracy: 0.9756 - val_loss: 0.7880 - val_accuracy: 0.8246\n",
      "Epoch 67/100\n",
      "208/208 [==============================] - 2s 12ms/step - loss: 0.0297 - accuracy: 0.9889 - val_loss: 0.8378 - val_accuracy: 0.8218\n",
      "Epoch 68/100\n",
      "208/208 [==============================] - 2s 12ms/step - loss: 0.0180 - accuracy: 0.9932 - val_loss: 0.8258 - val_accuracy: 0.8302\n",
      "Epoch 69/100\n",
      "208/208 [==============================] - 2s 12ms/step - loss: 0.0129 - accuracy: 0.9961 - val_loss: 0.8559 - val_accuracy: 0.8288\n",
      "Epoch 70/100\n",
      "208/208 [==============================] - 2s 12ms/step - loss: 0.0107 - accuracy: 0.9965 - val_loss: 0.8889 - val_accuracy: 0.8246\n",
      "Epoch 71/100\n",
      "208/208 [==============================] - 2s 12ms/step - loss: 0.0132 - accuracy: 0.9952 - val_loss: 0.9110 - val_accuracy: 0.8246\n",
      "Epoch 72/100\n",
      "208/208 [==============================] - 2s 12ms/step - loss: 0.0194 - accuracy: 0.9937 - val_loss: 0.9562 - val_accuracy: 0.8190\n",
      "Epoch 73/100\n",
      "208/208 [==============================] - 2s 11ms/step - loss: 0.0277 - accuracy: 0.9913 - val_loss: 0.8890 - val_accuracy: 0.8274\n",
      "Epoch 74/100\n",
      "208/208 [==============================] - 2s 12ms/step - loss: 0.0250 - accuracy: 0.9914 - val_loss: 0.9670 - val_accuracy: 0.8148\n",
      "Epoch 75/100\n",
      "208/208 [==============================] - 2s 11ms/step - loss: 0.0568 - accuracy: 0.9785 - val_loss: 0.9474 - val_accuracy: 0.8134\n",
      "Epoch 76/100\n",
      "208/208 [==============================] - 2s 11ms/step - loss: 0.0425 - accuracy: 0.9845 - val_loss: 0.9719 - val_accuracy: 0.8036\n",
      "Epoch 77/100\n",
      "208/208 [==============================] - 2s 11ms/step - loss: 0.0254 - accuracy: 0.9920 - val_loss: 0.9356 - val_accuracy: 0.8260\n",
      "Epoch 78/100\n",
      "208/208 [==============================] - 2s 12ms/step - loss: 0.0177 - accuracy: 0.9931 - val_loss: 0.9548 - val_accuracy: 0.8260\n",
      "Epoch 79/100\n",
      "208/208 [==============================] - 2s 12ms/step - loss: 0.0126 - accuracy: 0.9958 - val_loss: 0.9799 - val_accuracy: 0.8309\n",
      "Epoch 80/100\n",
      "208/208 [==============================] - 2s 11ms/step - loss: 0.0102 - accuracy: 0.9961 - val_loss: 0.9657 - val_accuracy: 0.8253\n",
      "Epoch 81/100\n",
      "208/208 [==============================] - 2s 12ms/step - loss: 0.0079 - accuracy: 0.9973 - val_loss: 0.9808 - val_accuracy: 0.8323\n",
      "Epoch 82/100\n",
      "208/208 [==============================] - 2s 12ms/step - loss: 0.0093 - accuracy: 0.9962 - val_loss: 0.9715 - val_accuracy: 0.8351\n",
      "Epoch 83/100\n",
      "208/208 [==============================] - 2s 12ms/step - loss: 0.0080 - accuracy: 0.9971 - val_loss: 0.9818 - val_accuracy: 0.8337\n",
      "Epoch 84/100\n",
      "208/208 [==============================] - 2s 11ms/step - loss: 0.0136 - accuracy: 0.9959 - val_loss: 0.9864 - val_accuracy: 0.8316\n",
      "Epoch 85/100\n",
      "208/208 [==============================] - 2s 11ms/step - loss: 0.0144 - accuracy: 0.9953 - val_loss: 0.9709 - val_accuracy: 0.8176\n",
      "Epoch 86/100\n",
      "208/208 [==============================] - 2s 11ms/step - loss: 0.0426 - accuracy: 0.9872 - val_loss: 0.9840 - val_accuracy: 0.8064\n",
      "Epoch 87/100\n",
      "208/208 [==============================] - 2s 12ms/step - loss: 0.0914 - accuracy: 0.9676 - val_loss: 0.8516 - val_accuracy: 0.8246\n",
      "Epoch 88/100\n",
      "208/208 [==============================] - 2s 12ms/step - loss: 0.0274 - accuracy: 0.9913 - val_loss: 0.8508 - val_accuracy: 0.8302\n",
      "Epoch 89/100\n",
      "208/208 [==============================] - 2s 11ms/step - loss: 0.0140 - accuracy: 0.9964 - val_loss: 0.9823 - val_accuracy: 0.8169\n",
      "Epoch 90/100\n",
      "208/208 [==============================] - 2s 11ms/step - loss: 0.0109 - accuracy: 0.9959 - val_loss: 1.0282 - val_accuracy: 0.8120\n",
      "Epoch 91/100\n",
      "208/208 [==============================] - 2s 12ms/step - loss: 0.0101 - accuracy: 0.9973 - val_loss: 1.0268 - val_accuracy: 0.8225\n",
      "Epoch 92/100\n",
      "208/208 [==============================] - 2s 11ms/step - loss: 0.0115 - accuracy: 0.9968 - val_loss: 1.0503 - val_accuracy: 0.8113\n",
      "Epoch 93/100\n",
      "208/208 [==============================] - 2s 12ms/step - loss: 0.0052 - accuracy: 0.9983 - val_loss: 1.0270 - val_accuracy: 0.8225\n",
      "Epoch 94/100\n",
      "208/208 [==============================] - 2s 12ms/step - loss: 0.0046 - accuracy: 0.9989 - val_loss: 1.0309 - val_accuracy: 0.8260\n",
      "Epoch 95/100\n",
      "208/208 [==============================] - 2s 11ms/step - loss: 0.0113 - accuracy: 0.9959 - val_loss: 1.0182 - val_accuracy: 0.8204\n",
      "Epoch 96/100\n",
      "208/208 [==============================] - 2s 12ms/step - loss: 0.0070 - accuracy: 0.9974 - val_loss: 1.0395 - val_accuracy: 0.8190\n",
      "Epoch 97/100\n",
      "208/208 [==============================] - 2s 11ms/step - loss: 0.0078 - accuracy: 0.9974 - val_loss: 1.0326 - val_accuracy: 0.8183\n",
      "Epoch 98/100\n",
      "208/208 [==============================] - 2s 12ms/step - loss: 0.0223 - accuracy: 0.9931 - val_loss: 1.1043 - val_accuracy: 0.7959\n",
      "Epoch 99/100\n",
      "208/208 [==============================] - 2s 12ms/step - loss: 0.0879 - accuracy: 0.9683 - val_loss: 1.0360 - val_accuracy: 0.8183\n",
      "Epoch 100/100\n",
      "208/208 [==============================] - 2s 12ms/step - loss: 0.0367 - accuracy: 0.9874 - val_loss: 0.9397 - val_accuracy: 0.8246\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You called `set_weights(weights)` on layer \"sequential_8\" with a weight list of length 7, but the layer was expecting 5 weights. Provided weights: [array([[[-1.44084498e-01,  1.38881996e-01,  3.185...",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 105\u001B[0m\n\u001B[0;32m    102\u001B[0m loaded_model_weights \u001B[38;5;241m=\u001B[39m pickle\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;28mopen\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m../model/best_model_weights_cnn_lstm.pkl\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[0;32m    104\u001B[0m \u001B[38;5;66;03m# 모델에 로드된 가중치 설정\u001B[39;00m\n\u001B[1;32m--> 105\u001B[0m \u001B[43mloaded_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mset_weights\u001B[49m\u001B[43m(\u001B[49m\u001B[43mloaded_model_weights\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    107\u001B[0m \u001B[38;5;66;03m# 모델 컴파일\u001B[39;00m\n\u001B[0;32m    108\u001B[0m loaded_model\u001B[38;5;241m.\u001B[39mcompile(loss\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcategorical_crossentropy\u001B[39m\u001B[38;5;124m'\u001B[39m, optimizer\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124madam\u001B[39m\u001B[38;5;124m'\u001B[39m, metrics\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\base_layer.py:1747\u001B[0m, in \u001B[0;36mLayer.set_weights\u001B[1;34m(self, weights)\u001B[0m\n\u001B[0;32m   1744\u001B[0m         expected_num_weights \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   1746\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m expected_num_weights \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(weights):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1748\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mYou called `set_weights(weights)` on layer \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m   1749\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwith a weight list of length \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m, but the layer was \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1750\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexpecting \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m weights. Provided weights: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m...\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1751\u001B[0m         \u001B[38;5;241m%\u001B[39m (\n\u001B[0;32m   1752\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname,\n\u001B[0;32m   1753\u001B[0m             \u001B[38;5;28mlen\u001B[39m(weights),\n\u001B[0;32m   1754\u001B[0m             expected_num_weights,\n\u001B[0;32m   1755\u001B[0m             \u001B[38;5;28mstr\u001B[39m(weights)[:\u001B[38;5;241m50\u001B[39m],\n\u001B[0;32m   1756\u001B[0m         )\n\u001B[0;32m   1757\u001B[0m     )\n\u001B[0;32m   1759\u001B[0m weight_index \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m   1760\u001B[0m weight_value_tuples \u001B[38;5;241m=\u001B[39m []\n",
      "\u001B[1;31mValueError\u001B[0m: You called `set_weights(weights)` on layer \"sequential_8\" with a weight list of length 7, but the layer was expecting 5 weights. Provided weights: [array([[[-1.44084498e-01,  1.38881996e-01,  3.185..."
     ]
    }
   ],
   "source": [
    "from tensorflow.python.estimator import keras\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import pickle\n",
    "\n",
    "# 데이터셋 준비\n",
    "train_df = pd.read_csv('../data/SisFall_train.csv')  # 훈련 파일\n",
    "test_df = pd.read_csv('../data/SisFall_test.csv')  # 테스트 파일\n",
    "\n",
    "X_train = []  # 훈련 입력 데이터\n",
    "y_train = []  # 훈련 출력 데이터\n",
    "\n",
    "X_test = []  # 테스트 입력 데이터\n",
    "y_test = []  # 테스트 출력 데이터\n",
    "\n",
    "# 4개의 레코드씩 묶어서 처리\n",
    "for i in range(0, len(train_df) - 39, 40):\n",
    "    # gx, gy, gz 값 가져오기\n",
    "    gx_values = train_df.loc[i:i+39, 'gx'].values\n",
    "    gy_values = train_df.loc[i:i+39, 'gy'].values\n",
    "    gz_values = train_df.loc[i:i+39, 'gz'].values\n",
    "\n",
    "    # 레이블 값 가져오기\n",
    "    labels = train_df.loc[i:i+39, 'label'].values\n",
    "\n",
    "    # 서로 다른 레이블이 포함된 경우 해당 시퀀스는 분석에서 제외\n",
    "    if len(set(labels)) == 1:\n",
    "        # 시퀀스로 변환하여 입력 데이터에 추가\n",
    "        X_train.append(np.transpose([gx_values, gy_values, gz_values]))\n",
    "\n",
    "        # 레이블 값 중복 제거하여 출력 데이터에 추가\n",
    "        y_train.append(labels[0])\n",
    "\n",
    "for i in range(0, len(test_df) - 39, 40):\n",
    "    # gx, gy, gz 값 가져오기\n",
    "    gx_values = test_df.loc[i:i+39, 'gx'].values\n",
    "    gy_values = test_df.loc[i:i+39, 'gy'].values\n",
    "    gz_values = test_df.loc[i:i+39, 'gz'].values\n",
    "\n",
    "    # 레이블 값 가져오기\n",
    "    labels = test_df.loc[i:i+39, 'label'].values\n",
    "\n",
    "    # 서로 다른 레이블이 포함된 경우 해당 시퀀스는 분석에서 제외\n",
    "    if len(set(labels)) == 1:\n",
    "        # 시퀀스로 변환하여 입력 데이터에 추가\n",
    "        X_test.append(np.transpose([gx_values, gy_values, gz_values]))\n",
    "\n",
    "        # 레이블 값 중복 제거하여 출력 데이터에 추가\n",
    "        y_test.append(labels[0])\n",
    "\n",
    "# 입력 데이터와 출력 데이터를 넘파이 배열로 변환\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# 출력 데이터를 One-Hot 인코딩\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded_train = label_encoder.fit_transform(y_train)\n",
    "integer_encoded_train = integer_encoded_train.reshape(len(integer_encoded_train), 1)\n",
    "onehot_encoder_train = OneHotEncoder(sparse_output=False)\n",
    "y_train = onehot_encoder_train.fit_transform(integer_encoded_train)\n",
    "\n",
    "integer_encoded_test = label_encoder.transform(y_test)\n",
    "integer_encoded_test = integer_encoded_test.reshape(len(integer_encoded_test), 1)\n",
    "onehot_encoder_test = OneHotEncoder(sparse_output=False)\n",
    "y_test = onehot_encoder_test.fit_transform(integer_encoded_test)\n",
    "\n",
    "# CNN-LSTM 모델 정의\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(40, 3)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# 모델 컴파일 및 학습\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# 최상의 적합 모델의 가중치를 피클 파일로 저장\n",
    "best_model_weights = history.model.get_weights()\n",
    "pickle.dump(best_model_weights, open('../model/best_model_weights_cnn_lstm.pkl', 'wb'))\n",
    "\n",
    "# 레이블 인코더의 클래스 정보를 저장\n",
    "np.save('../model/label_encoder_classes_cnn_lstm.npy', label_encoder.classes_)\n",
    "\n",
    "# 가중치를 로드하기 위해 모델 구성\n",
    "loaded_model = keras.models.Sequential()\n",
    "loaded_model.add(keras.layers.LSTM(128, input_shape=(3, 40)))  # 3개의 시퀀스, 각 시퀀스에 40개의 피처\n",
    "loaded_model.add(keras.layers.Dense(7, activation='softmax'))  # 분류할 클래스 수에 맞게 조정\n",
    "\n",
    "# 피클 파일에서 가중치 로드\n",
    "loaded_model_weights = pickle.load(open('../model/best_model_weights_cnn_lstm.pkl', 'rb'))\n",
    "\n",
    "# 모델에 로드된 가중치 설정\n",
    "loaded_model.set_weights(loaded_model_weights)\n",
    "\n",
    "# 모델 컴파일\n",
    "loaded_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 모델 저장\n",
    "loaded_model.save('../model/loaded_model_cnn_lstm.h5')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T05:54:06.055626400Z",
     "start_time": "2023-06-12T05:50:02.497408700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mplt\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# Get the training history\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m train_loss \u001B[38;5;241m=\u001B[39m \u001B[43mhistory\u001B[49m\u001B[38;5;241m.\u001B[39mhistory[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mloss\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m      5\u001B[0m train_acc \u001B[38;5;241m=\u001B[39m history\u001B[38;5;241m.\u001B[39mhistory[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m      6\u001B[0m val_loss \u001B[38;5;241m=\u001B[39m history\u001B[38;5;241m.\u001B[39mhistory[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval_loss\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "\u001B[1;31mNameError\u001B[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the training history\n",
    "train_loss = history.history['loss']\n",
    "train_acc = history.history['accuracy']\n",
    "val_loss = history.history['val_loss']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "# Plot the training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_loss, label='Train Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.ylim([0, 1])\n",
    "plt.show()\n",
    "\n",
    "# Plot the training and validation accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_acc, label='Train Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.ylim([0, 1])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T05:22:21.469963700Z",
     "start_time": "2023-06-12T05:22:21.173002500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
